<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>数据科学导论-2：数据统计</title>
      <link href="/2019/10/08/data-science-2/"/>
      <url>/2019/10/08/data-science-2/</url>
      
        <content type="html"><![CDATA[<h1 id="数据统计"><a href="#数据统计" class="headerlink" title="数据统计"></a>数据统计</h1><p><img alt data-src class="lozad"></p><h2 id="概率分布"><a href="#概率分布" class="headerlink" title="概率分布"></a>概率分布</h2><ul><li>概率分布是统计模型的基础。</li><li>概率分布用以描述随机变量取值的概率规律—随机变量所有可能的取值以及取每个值所对应的概率。</li><li>注意：离散型随机变量和连续型随机变量的概率分布的描述方法不同。</li></ul><h3 id="离散型随机变量的概率分布"><a href="#离散型随机变量的概率分布" class="headerlink" title="离散型随机变量的概率分布"></a>离散型随机变量的概率分布</h3><p><img alt data-src="https://pic.superbed.cn/item/5d9c78b0451253d17854ac66.png" class="lozad"><br><img alt data-src="https://pic.superbed.cn/item/5d9c78a9451253d17854ab81.png" class="lozad"></p><h3 id="连续性随机变量的概率分布"><a href="#连续性随机变量的概率分布" class="headerlink" title="连续性随机变量的概率分布"></a>连续性随机变量的概率分布</h3><p>与离散型随机变量不同的是，连续型随机变量的所有可能取值具有无法穷举性，因此其概率分布一般用概率密度函数描述。<br><img alt data-src="https://pic.superbed.cn/item/5d9c78a5451253d17854aae6.png" class="lozad"><br>概率密度函数f（x）：设x为一连续型随机变量，x 为任意实数，X的概率密度函数记为f(x)，也就是说概率密度函数 f(x)表示X的所有取值 x 及其频数f(x)</p><h4 id="正态分布"><a href="#正态分布" class="headerlink" title="正态分布"></a>正态分布</h4><p>正态分布是描述连续型随机变量的最重要的分布，它是经典统计推断的基础。<br>如果随机变量X的概率密度函数 f(x)为<br><img alt data-src="https://pic.superbed.cn/item/5d9c7931451253d17854e4ad.png" class="lozad"><br>那么，称X服从正态分布，并记作X~N(μ,σ2)，式中: f(x) = 随机变量X的频数；μ=总体均值；σ2 = 总体方差（sigma）；π = 3.1459; e = 2.7182。<br><img alt data-src="https://pic.superbed.cn/item/5d9c79b0451253d17854f30b.png" class="lozad"><br><img alt data-src="https://pic.superbed.cn/item/5d9c7a04451253d178551022.png" class="lozad"><br><img alt data-src="https://pic.superbed.cn/item/5d9c79fe451253d178550b39.png" class="lozad"></p><h2 id="参数估计与假设检验"><a href="#参数估计与假设检验" class="headerlink" title="参数估计与假设检验"></a>参数估计与假设检验</h2><ul><li>参数估计：参数估计是指用样本指标（称为统计量）估计总体指标（称为参数）。</li><li>假设检验：<ul><li>假设就是对总体参数的具体表述。</li><li>一个假设的提出是以一定理由为基础的，但这些理由通常是不完全充分的，因此就需要“检验”</li><li>例，新配方药物疗效是否比原配方要好？<br><img alt data-src="https://pic.superbed.cn/item/5d9c7a97451253d1785539a1.png" class="lozad"></li></ul></li></ul><h3 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h3><ul><li>点估计</li><li>区间估计</li></ul><h4 id="点估计"><a href="#点估计" class="headerlink" title="点估计"></a>点估计</h4><ul><li>基本思路：从总体中抽取一个样本，根据该样本的统计量对总体的未知参数做出一个数值点的估计</li><li>例如：用样本均值作为总体未知均值μ的估计值<ul><li>例：“X公司工资多少啊？”“我几个师兄平均20K吧。”</li></ul></li><li>注意：点估计没有给出估计值接近总体未知参数程度的信息</li></ul><h4 id="区间估计"><a href="#区间估计" class="headerlink" title="区间估计"></a>区间估计</h4><ul><li>区间估计<ul><li>在点估计的基础上，给出总体参数落在某一区间的概率。</li><li>区间就是根据一个样本的观察值给出的总体参数的估计范围，可通过样本统计量加减抽样误差的方法计算。</li><li>例如，总体均值落在50~70之间的概率为 0.95。<br><img alt data-src="https://pic.superbed.cn/item/5d9c7b63451253d178557e74.png" class="lozad"></li></ul></li><li>区间估计的指标<ul><li>置信区间（Confidence interval）<ul><li>是指由样本统计量所构造的总体参数的估计区间，其中区间的最小值和最大值分别称为置信下限和置信上限。</li></ul></li><li>置信水平（Confidence lvel）<ul><li>是指总体未知参数落在区间内的概率，表示为 (1–α)％，α为显著性水平，即总体参数未在区间内的概率。</li><li>在构造置信区间时，可以任意设置目标置信水平。常用的置信水平及正态分布曲线下右侧面积为α/2时的z值 （za/2）。<br><img alt data-src="https://pic.superbed.cn/item/5d9c7bf1451253d178558d47.png" class="lozad"></li></ul></li></ul></li><li>参数估计中，用于估计总体某一参数的随机变量称为估计量（estimator），如样本均值就是总体均值μ的一个估计量。<ul><li>判断估计量的优良性准则：<ul><li>无偏性（Unbiasednes）<ul><li>估计量的数学期望等于被估计的总体参数</li></ul></li><li>有效性(Eficency)<ul><li>一个方差较小的无偏估计量称为一个更有效的估计量。如，与其他估计量相比，样本均值是一个更有效的估计量</li></ul></li><li>一致性(Consitency)<ul><li>随着样本容量的增大，估计量越来越接近被估计的总体参数。</li></ul></li></ul></li></ul></li></ul><h2 id="假设检验"><a href="#假设检验" class="headerlink" title="假设检验"></a>假设检验</h2><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据科学导论 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据科学导论-1：探索性数据分析和数据预处理</title>
      <link href="/2019/10/08/data-science-1/"/>
      <url>/2019/10/08/data-science-1/</url>
      
        <content type="html"><![CDATA[<h1 id="探索性数据分析和数据预处理"><a href="#探索性数据分析和数据预处理" class="headerlink" title="探索性数据分析和数据预处理"></a>探索性数据分析和数据预处理</h1><h2 id="建模"><a href="#建模" class="headerlink" title="建模"></a>建模</h2><h3 id="什么是模型"><a href="#什么是模型" class="headerlink" title="什么是模型"></a>什么是模型</h3><h3 id="统计建模"><a href="#统计建模" class="headerlink" title="统计建模"></a>统计建模</h3><h3 id="如何构建模型"><a href="#如何构建模型" class="headerlink" title="如何构建模型"></a>如何构建模型</h3><ul><li>探索性数据分析（EDA）</li><li>概率分布是统计模型的基础</li><li>拟合模型</li><li>要小心“过拟合”</li></ul><h2 id="探索性数据分析"><a href="#探索性数据分析" class="headerlink" title="探索性数据分析"></a>探索性数据分析</h2><p>探索性数据分析是建模的第一步<br>验证性数据分析偏重于模型和假设，探索性数据分析<strong>没有假设</strong>，也没有模型<br>探索性数据分析是数据分析的开端，数据可视化是数据分析的最后一个环节</p><h3 id="探索性数据分析做什么？"><a href="#探索性数据分析做什么？" class="headerlink" title="探索性数据分析做什么？"></a>探索性数据分析做什么？</h3><p>探索性数据分析是一种系统性分析数据的方法，包括</p><ul><li>展示所有变量的分布情况（利用<strong><red>盒型图</red></strong>）、时间序列数据和变换变量</li><li>利用<strong><red>散点矩阵图</red></strong>展示变量两两之间的关系</li><li>得到所有的汇总统计量：<strong><red>均值</red></strong>、<strong><red>最大值</red></strong>、<strong><red>最小值</red></strong>、<strong><red>上下四分位数</red></strong>和确定<strong><red>异常值</red></strong></li></ul><h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p><img alt="数据质量的属性" data-src="https://pic.superbed.cn/item/5d9c64a6451253d1784f1cb9.png" class="lozad"><br><img alt="数据预处理方法" data-src="https://pic.superbed.cn/item/5d9c6a26451253d178504c65.png" class="lozad"></p><h2 id="数据审计"><a href="#数据审计" class="headerlink" title="数据审计"></a>数据审计</h2><p>数据审计是按照数据质量的一般规律与评估方法，对数据内容及元数据进行审计，发现其中存在的“问题”，如：</p><ul><li>缺失值（缺少数据）</li><li>噪声值（异常数据）</li><li>不一致值（相互矛盾的数据）</li><li>不完整值（被篡改或无法溯源的数据）</li></ul><p><strong>数据审计=可视化审计+自定义审计+预定义审计</strong></p><h3 id="预定义审计"><a href="#预定义审计" class="headerlink" title="预定义审计"></a>预定义审计</h3><p>可以依赖的数据或方法：</p><ul><li>数据字典</li><li>用户自定义的完整性约束条件</li><li>数据的自描述性信息</li><li>属性的定义域与值域</li><li>数据自包含的关联关系</li></ul><h3 id="自定义审计"><a href="#自定义审计" class="headerlink" title="自定义审计"></a>自定义审计</h3><ul><li>变量定义规则<ul><li>给出一个有效值（或无效值）的取值范围</li><li>列出所有有效值（或无效值），以有效值（无效值列表）形式定义</li></ul></li><li>函数定义规则<ul><li>相对于简单变量定义规则，函数定义规则更为复杂，需要对变量进行函数计算</li><li>例如，设计一个函数f(),并定义规则f(age)=TRUE</li></ul></li></ul><h3 id="可视化审计"><a href="#可视化审计" class="headerlink" title="可视化审计"></a>可视化审计</h3><p>数据可视化可能很容易的发现某些问题</p><h3 id="数据审计与数据清洗"><a href="#数据审计与数据清洗" class="headerlink" title="数据审计与数据清洗"></a>数据审计与数据清洗</h3><p><img alt="数据审计与数据清洗" data-src="https://pic.superbed.cn/item/5d9c6d5d451253d1785118bb.png" class="lozad"></p><h3 id="缺失数据处理"><a href="#缺失数据处理" class="headerlink" title="缺失数据处理"></a>缺失数据处理</h3><p>缺失数据的识别：数据审计–&gt;<br>缺失数据分析：特征分析、影响分析、原因分析–&gt;<br>缺失数据处理：忽略、删除、插值<br><img alt="缺失数据处理" data-src="https://pic.superbed.cn/item/5d9c6eb6451253d1785158b6.png" class="lozad"></p><h4 id="缺失值类型"><a href="#缺失值类型" class="headerlink" title="缺失值类型"></a>缺失值类型</h4><p><img alt="缺失值类型" data-src="https://pic.superbed.cn/item/5d9c6ebb451253d17851594d.png" class="lozad"></p><h3 id="冗余数据处理"><a href="#冗余数据处理" class="headerlink" title="冗余数据处理"></a>冗余数据处理</h3><p>识别冗余数据：数据审计–&gt;<br>冗余数据的分析：重复数据、无关数据–&gt;<br>冗余数据的过滤：重复过滤、条件过滤<br><img alt="冗余数据处理" data-src="https://pic.superbed.cn/item/5d9c6eb9451253d1785158f3.png" class="lozad"></p><h3 id="噪声数据处理"><a href="#噪声数据处理" class="headerlink" title="噪声数据处理"></a>噪声数据处理</h3><h4 id="噪声数据的存在形式"><a href="#噪声数据的存在形式" class="headerlink" title="噪声数据的存在形式"></a>噪声数据的存在形式</h4><ul><li>错误数据</li><li>虚假数据</li><li>异常数据</li><li>离群数据或孤立数据</li></ul><h4 id="噪声数据的处理方法"><a href="#噪声数据的处理方法" class="headerlink" title="噪声数据的处理方法"></a>噪声数据的处理方法</h4><ul><li>分箱（Binning）</li><li>聚类（Clustering）</li><li>回归（Regression）</li></ul><h4 id="分箱处理的步骤与类型"><a href="#分箱处理的步骤与类型" class="headerlink" title="分箱处理的步骤与类型"></a>分箱处理的步骤与类型</h4><p><img alt="分箱处理的步骤与类型" data-src="https://pic.superbed.cn/item/5d9c70a6451253d17851be2d.png" class="lozad"><br><img alt="均值平滑和边界值平滑" data-src="https://pic.superbed.cn/item/5d9c70a8451253d17851c067.png" class="lozad"><br><img alt="通过聚类发现离群点/孤立点" data-src="https://pic.superbed.cn/item/5d9c70ad451253d17851c545.png" class="lozad"><br><img alt="通过回归方法发现噪声数据" data-src="https://pic.superbed.cn/item/5d9c70aa451253d17851c300.png" class="lozad"></p><h2 id="数据变换"><a href="#数据变换" class="headerlink" title="数据变换"></a>数据变换</h2><h3 id="数据变换的类型"><a href="#数据变换的类型" class="headerlink" title="数据变换的类型"></a>数据变换的类型</h3><p>方法：</p><ul><li>平滑处理：去除噪声数据</li><li>特征构造：构造出新的特征</li><li>聚集：进行粗粒度计算</li><li>标准化：将特征（属性）按比例缩放，是指落入一个特定的区间</li><li>离散化：用区间或概念标签表示数据</li></ul><h3 id="大小变换"><a href="#大小变换" class="headerlink" title="大小变换"></a>大小变换</h3><p>数据标准化：</p><ul><li>0-1标准化：<ul><li>对原始数据的线性变换，使结果落入[0,1]区间，<img alt="转换函数" data-src="https://pic.superbed.cn/item/5d9c72e7451253d178525639.png" class="lozad"></li><li>其中，<ul><li>max和max分别为样本数据的最大值和最小值</li><li>x与x* 分别代表标准化处理前的值和标准化处理后的值</li></ul></li><li>缺点<ul><li>当有新数据加入时，可能导致最大值和最小值的变化，需要重新定义min和max的取值。</li></ul></li></ul></li><li>z-score标准化<ul><li>经过处理的数据符合标准正态分布，即均值为0，标准差为1，<img alt="转换函数" data-src="https://pic.superbed.cn/item/5d9c72ea451253d17852591d.png" class="lozad"></li><li>其中，<ul><li>μ为平均数</li><li>σ为标准差</li><li>x与z分别代表标准化处理前的值和标准化处理后的值</li></ul></li></ul></li></ul><h3 id="类型变换"><a href="#类型变换" class="headerlink" title="类型变换"></a>类型变换</h3><ul><li>一对一变换</li><li>多对一变换</li></ul><h2 id="数据集成"><a href="#数据集成" class="headerlink" title="数据集成"></a>数据集成</h2><ul><li>内容集成<br><img alt="内容集成" data-src="https://pic.superbed.cn/item/5d9c745a451253d1785331e4.png" class="lozad"></li><li>结构集成<br><img alt="结构集成" data-src="https://pic.superbed.cn/item/5d9c745d451253d178533231.png" class="lozad"></li></ul><h3 id="数据集成的三个问题"><a href="#数据集成的三个问题" class="headerlink" title="数据集成的三个问题"></a>数据集成的三个问题</h3><ul><li>模式集成</li><li>数据冗余</li><li>冲突检测与消除</li></ul><h2 id="数据脱敏"><a href="#数据脱敏" class="headerlink" title="数据脱敏"></a>数据脱敏</h2><p><img alt="数据脱敏" data-src="https://pic.superbed.cn/item/5d9c74f2451253d178536436.png" class="lozad"></p><h3 id="数据脱敏的原则"><a href="#数据脱敏的原则" class="headerlink" title="数据脱敏的原则"></a>数据脱敏的原则</h3><ul><li>单向性</li><li>无残留</li><li>易实现</li></ul><h2 id="数据归约"><a href="#数据归约" class="headerlink" title="数据归约"></a>数据归约</h2><p>数据规约是指在不影响数据完整性和数据分析结果正确性的前提下，通过减少数据规模的方式达到提升数据分析的效果和效率的目的。</p><ul><li>维归约<ul><li>主成分分析</li><li>奇异值分析</li><li>离散小波转换</li></ul></li><li>值归约<ul><li>参数模型（简单线性回归模型和对数线性模型等）</li><li>非参数模型（抽样、聚类、直方图等）</li></ul></li></ul><h2 id="数据标注"><a href="#数据标注" class="headerlink" title="数据标注"></a>数据标注</h2><p>数据标注是通过对目标数据补充必要的关键字、语义信息等标签，提高数据分析和挖掘的效果和效率</p><ul><li>按标注活动的自动化程度，数据标注可以分为手工标注、自动化标注和半自动化标注</li><li>从标注的实现层次看，数据标注可以分为：<ul><li>语法标注（文本数据的词性、句法等）</li><li>语义标注（数据的主题、情感、意见等）</li></ul></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据科学导论 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hadoop离线计算-2：Hadoop分布式文件存储系统</title>
      <link href="/2019/09/27/hadoop-2/"/>
      <url>/2019/09/27/hadoop-2/</url>
      
        <content type="html"><![CDATA[<h1 id="Hadoop分布式文件存储系统"><a href="#Hadoop分布式文件存储系统" class="headerlink" title="Hadoop分布式文件存储系统"></a>Hadoop分布式文件存储系统</h1><h2 id="hadoop简介"><a href="#hadoop简介" class="headerlink" title="hadoop简介"></a>hadoop简介</h2><h3 id="官方定义："><a href="#官方定义：" class="headerlink" title="官方定义："></a>官方定义：</h3><p>分布式存储系统HDFS（Hadoop Distributed File System）。<br>其实就是一个文件系统，类似于linux的文件系统。有目录，目录下可以存储文件。但它又是一个分布式的文件系统。</p><h3 id="基本原理："><a href="#基本原理：" class="headerlink" title="基本原理："></a>基本原理：</h3><ul><li>将文件切分成等大的数据块。分别存储到多台机器上。</li><li>每个数据块存在多个备份。</li><li>将数据切分、容错、负载均衡等功能透明化。</li><li>可将HDFS看成是一个巨大、具有容错性的磁盘。</li></ul><h2 id="Hadoop实现原理"><a href="#Hadoop实现原理" class="headerlink" title="Hadoop实现原理"></a>Hadoop实现原理</h2><h3 id="基本原理：-1"><a href="#基本原理：-1" class="headerlink" title="基本原理："></a>基本原理：</h3><ul><li>将文件切分成等大的数据块。分别存储到多台机器上。</li><li>每个数据块存在多个备份。</li><li>将数据切分、容错、负载均衡等功能透明化。</li><li>可将HDFS看成是一个巨大、具有容错性的磁盘。</li></ul><h3 id="优点："><a href="#优点：" class="headerlink" title="优点："></a>优点：</h3><ul><li>处理超大文件</li><li>流式的数据访问</li><li>运行于廉价的商业机器集群上</li></ul><h3 id="缺点："><a href="#缺点：" class="headerlink" title="缺点："></a>缺点：</h3><ul><li>不适合存储大量小文件</li><li>不适合低延迟数据访问</li><li>不支持多用户写入及任意修改文件</li></ul><p><img alt="Hadoop实现原理" data-src="https://pic.superbed.cn/item/5d8d6021451253d17830f971.png" class="lozad"></p><h2 id="HDFS实现原理-namenode和datanode"><a href="#HDFS实现原理-namenode和datanode" class="headerlink" title="HDFS实现原理-namenode和datanode"></a>HDFS实现原理-namenode和datanode</h2><p>HDFS集群有两类节点，并以管理者-工作者模式运行，即一个namenode(管理者)和多个datanode(工作者)。 </p><ul><li><strong>namenode</strong>： namenode管理文件系统的命名空间。它维护着文件系统树及整棵树内所有的文件和目录。这些信息以两个文件形式永久保存在本地磁盘上:命名空间镜像文件(fs-image)和编辑日志(edit-logs)文件。namenode也记录着每个文件中各个块所在的数据节点信息，但它并不永久保存块的位置信息，因为这些信息会在系统启动时由数据节点重建。 </li><li><strong>datanode</strong>： 是文件系统的工作节点。它们根据需要存储并检索数据块(受客户端或namenode调度)，并且定期向namenode 发送它们所存储的块的列表。 </li><li><strong>客户端(Client)</strong>： Client代表用户通过namenode和datanode访问整个文件系统。客户端提供一个类似于POSIX（可移植操作系统界面）的文件系统接口，因此用户在编程时无需知道namenode和datanode也可实现其功能。</li><li><strong>Namenode备份机制</strong>：<ul><li>文件系统元数据持久状态的文件备份：一般的配置是，将持久状态写入本地磁盘的同时，写入一个远程挂载的网络文件系统 (NFS)。</li><li>nameNodeHA。</li></ul></li></ul><h2 id="HDFS实现原理"><a href="#HDFS实现原理" class="headerlink" title="HDFS实现原理"></a>HDFS实现原理</h2><h3 id="数据块"><a href="#数据块" class="headerlink" title="数据块:"></a>数据块:</h3><ul><li>每个磁盘都有默认的数据块大小，这是磁盘进行数据读/写的最小单位。构建于单个磁盘之上的文件系统通过磁盘块来管理该文件系统中的块，该文件系统块的大小可以是磁盘块的整数倍。</li><li>HDFS同样也有块 (block)的概念，但是大得多，默认为128 MB 。与单一磁盘上的文件系统相 似，HDFS上的文件也被划分为块大小的多个分块 (chunk) ，作为独立的存储单元。但与其他 文件系统不同的是，HDFS中小于一个块大小的文件不会占据整个块的空间。<br><img alt="example" data-src="https://pic.superbed.cn/item/5d8d6331451253d178319ea2.png" class="lozad"></li></ul><h3 id="优点：-1"><a href="#优点：-1" class="headerlink" title="优点："></a>优点：</h3><ul><li>一个大文件不用存储于整块磁盘上，可以分布式存储。</li><li>使用块抽象而非整个文件作为存储单元，大大简化了存储子系统的设计。这对于故障种类繁 多的分布式系统尤为重要。 </li></ul><p>与磁盘管理相似，HDFS提供了fsck命令可以显示块信息：<br> hdfs fsck / -files -blocks</p><h3 id="NameNode"><a href="#NameNode" class="headerlink" title="NameNode"></a>NameNode</h3><ul><li>NameNode是HDFS架构中的主节点。</li></ul><h4 id="功能"><a href="#功能" class="headerlink" title="功能"></a>功能</h4><ul><li>管理各个从节点的状态(DataNode)。 </li><li>记录存储在HDFS上的所有数据的元数据信息。例如：block存储的位置，文件大小，文件权限， 文件层级等等。这些信息以两个文件形式永久保存在本地磁盘上。 <ul><li><strong>命名空间镜像文件(FsImage)</strong>: fsimage是HDFS文件系统存于硬盘中的元数据检查点，里面记录 了自最后一次检查点之前HDFS文件系统中所有目录和文件的序列化信息 </li><li><strong>编辑日志(edit-logs)文件</strong>：保存了自最后一次检查点之后所有针对HDFS文件系统的操作，比如： 增加文件、重命名文件、删除目录等等。 </li></ul></li><li>记录了存储在HDFS上文件的所有变化，例如文件被删除，namenode会记录到editlog中。 </li><li>接受DataNode的心跳和各个datanode上的block报告信息，确保DataNode是否存活。</li><li>负责处理所有块的复制因子。</li><li>如果DataNode节点宕机，NameNode会选择另外一个DataNode均衡复制因子，并做负载均衡。<br>参考：<a href="http://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html" target="_blank" rel="noopener">http://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html</a></li></ul><h3 id="Secondary-NameNode"><a href="#Secondary-NameNode" class="headerlink" title="Secondary NameNode"></a>Secondary NameNode</h3><ul><li>SNameNode是NameNode的助手，不要将其理解成是NameNode的备份。Secondary NameNode 的整个目的在HDFS中提供一个Checkpoint Node，所以也被叫做checkpoint node。 </li></ul><p><strong>不与NameNode部署在同一节点，为了减少NameNode负担</strong></p><h4 id="功能-1"><a href="#功能-1" class="headerlink" title="功能"></a>功能</h4><ul><li>定时的从NameNode获取EditLogs,并更新到FsImage上。</li><li>一旦它有新的fsimage文件，它将其拷贝回NameNode上，NameNode在下次重启时会使用这个新的fsimage文件，从而减少重启的时间。<br><img alt="Hadoop实现原理-1" data-src="https://pic.superbed.cn/item/5d8d65bd451253d178323a5a.png" class="lozad"></li></ul><p><font color="red">注意：关于NameNode是什么时候将改动写到editlogs中的？这个操作实际上是由DataNode的写操作触发的，当我们往DataNode写文件时，DataNode会跟NameNode通信，告诉NameNode什么文件的第几个block放在它那里，NameNode这个时候会将这些元数据信息写到editlogs文件中</font></p><h3 id="DataNode"><a href="#DataNode" class="headerlink" title="DataNode"></a>DataNode</h3><ul><li>DataNode是HDFS架构的从节点，管理各自节点的Block信息。 </li></ul><h4 id="功能-2"><a href="#功能-2" class="headerlink" title="功能"></a>功能</h4><ul><li>多个数据实际是存储到DataNode上面。</li><li>DataNode分别运行在独立的节点上。</li><li>DataNode执行客户端级别的读写请求。客户端直接和DataNode交互</li><li>向NameNode发送心跳(默认是3s)，报告各自节点的健康状况。</li></ul><h3 id="复制因子-block"><a href="#复制因子-block" class="headerlink" title="复制因子(block)"></a>复制因子(block)</h3><ul><li>HDFS为我们提供了可靠的存储，就是因为这个复制因子。默认复制因子是3。<br><img alt="复制因子" data-src="https://pic.superbed.cn/item/5d8d6927451253d178330b85.png" class="lozad"></li></ul><p><strong>注意： DataNode会定时发送心跳给NameNode，汇报各自节点的Block 信息。NameNode收集到这些信息后，会对超出复制因子的Block删除，复制因子不足的Block做添加。</strong></p><h3 id="机架感知"><a href="#机架感知" class="headerlink" title="机架感知"></a>机架感知</h3><p><img alt="机架感知" data-src="https://pic.superbed.cn/item/5d8d69e1451253d178333608.png" class="lozad"></p><ul><li>分布式的集群通常包含非常多的机器，由于受到机架槽位和交换机网口的限制，通常大型的分布式集群都会跨好几个机架，由多个机架上的机器共同组成一个分布式集群。机架内的机器之间的网络速度通常都会高于跨机架机器之间的网络速度，并且机架之间机器的网络通信通常受到上层交换机间网络带宽的限制。</li></ul><h2 id="HDFS读写流程-写数据"><a href="#HDFS读写流程-写数据" class="headerlink" title="HDFS读写流程-写数据"></a>HDFS读写流程-写数据</h2><p>假设我们有一个example.txt的文件，大小为248M。要将其写入到HDFS中。假设我们使用HDFS设置的块大小为128M(默认值)。那么client会将此文件拆分成两个块。第一个块是120MB(BlOCK 1),第二个块是120MB(BLOCK 2)。<br><img alt="example" data-src="https://pic.superbed.cn/item/5d8d6a6b451253d178335aab.png" class="lozad"></p><ul><li>首先，client请求namenode，要将多个块写入到HDFS。例如这里的Block A和Block B。</li><li>NameNode会给client赋予写权限，并为client提供可以写入数据的DataNode的IP地址。Namenode在选择可写入数据的dataNode的规则是结合了DN的健康状态、复制因子、机架感知等因素随机选择的DN。假如复制因子是3(默认值），那么会为每个block返回三个IP地址。例如NN为client提供了以下的IP地址列表。<br>Block A, list A = {IP of DataNode 1, IP of DataNode 4, IP of DataNode 6}<br>Block B, set B = {IP of DataNode 3, IP of DataNode 7, IP of DataNode 9} </li><li>整体的数据复制流程分一下三个阶段。1.流水线建立；2.复制数据；3.关闭流水线；<br><img alt="数据复制流程" data-src="https://pic.superbed.cn/item/5d8d6b4c451253d1783396f9.png" class="lozad"></li></ul><h2 id="HDFS读写流程-写数据之建立流水线（pipline）"><a href="#HDFS读写流程-写数据之建立流水线（pipline）" class="headerlink" title="HDFS读写流程-写数据之建立流水线（pipline）"></a>HDFS读写流程-写数据之建立流水线（pipline）</h2><p><img alt="建立流水线" data-src="https://pic.superbed.cn/item/5d8d6c82451253d17833e489.png" class="lozad"></p><ul><li>在写入数据之前，client首先要确认namenode提供的ip列表是否准备好了接收数据。Client通过连接各个块的ip列表来为每个块创建流水线。</li><li>我们以Block1为例。<br>For Block A, list A = {<br>IP of DataNode 1,<br>IP of DataNode 4,<br>IP of DataNode 6}</li></ul><h2 id="HDFS读写流程-写数据之数据复制（data-Streaming"><a href="#HDFS读写流程-写数据之数据复制（data-Streaming" class="headerlink" title="HDFS读写流程-写数据之数据复制（data Streaming)"></a>HDFS读写流程-写数据之数据复制（data Streaming)</h2><p><img alt="数据复制" data-src="https://pic.superbed.cn/item/5d8d6da9451253d178343d0d.png" class="lozad"></p><ul><li>流水线建立好之后，client将会向流水线中写入数据。</li><li>注意：Client只会将block A向DN1复制。其他节点复制是在DN之间完成的。</li></ul><h2 id="HDFS读写流程-写数据之关闭流水线"><a href="#HDFS读写流程-写数据之关闭流水线" class="headerlink" title="HDFS读写流程-写数据之关闭流水线"></a>HDFS读写流程-写数据之关闭流水线</h2><p><img alt="关闭流水线" data-src="https://pic.superbed.cn/item/5d8d6e22451253d178345cf9.png" class="lozad"></p><ul><li>当数据复制到所有的DN完成之后，按照ip地址列表相反的方向，依次反馈写入成功的信息。</li><li>DN6-&gt;DN4-&gt;DN1</li><li>DN1将确认信息反馈给client， client再将确认信息反馈给NN，NN更新元数据信息，client关<br>闭pipline。</li></ul><h2 id="HDFS读写流程-多个Block同时写入"><a href="#HDFS读写流程-多个Block同时写入" class="headerlink" title="HDFS读写流程-多个Block同时写入"></a>HDFS读写流程-多个Block同时写入</h2><p><img alt="多个Block同时写入" data-src="https://pic.superbed.cn/item/5d8d6edb451253d17834888c.png" class="lozad"></p><ul><li>Block A和Block B的写入是并行进行的。</li><li>Block A:1A-&gt;2A-&gt;3A</li><li>Block B:1B-&gt;2B-&gt;3B-&gt;4B-&gt;5B-&gt;6B</li></ul><h2 id="HDFS读写流程-文件读取"><a href="#HDFS读写流程-文件读取" class="headerlink" title="HDFS读写流程-文件读取"></a>HDFS读写流程-文件读取</h2><p><img alt="文件读取" data-src="https://pic.superbed.cn/item/5d8d6f98451253d17834baa5.png" class="lozad"></p><ul><li>Client请求namenode要读取exaple.txt文件。</li><li>NN根据自己的元数据信息，反馈给client一个DataNode的列表(存储Block A和B)。</li><li>Client连接DN，读取BlockA,Block B的数据。</li><li>Client合并block A和Block B的数据。</li></ul><h2 id="HDFS-filesystem-shell"><a href="#HDFS-filesystem-shell" class="headerlink" title="HDFS filesystem shell"></a>HDFS filesystem shell</h2><h3 id="cat"><a href="#cat" class="headerlink" title="[cat]"></a>[cat]</h3><p>使用方法：hadoop fs -cat URI [URI …]<br>说明：将路径指定文件的内容输出到<em>stdout</em></p><ul><li>hadoop fs -cat /dataAnalysis/duliming/input/mapreduceInput/wordCount.txt </li><li>hadoop fs -cat file:///usr/local/usrJar/duliming/DataAnalysis/output/AEP/20161203094310.csv</li></ul><h3 id="appendToFile"><a href="#appendToFile" class="headerlink" title="[appendToFile]"></a>[appendToFile]</h3><p>使用方法：hadoop fs -appendToFile <localsrc> … <dst><br>说明：添加(追加)一个或多个源文件到目标文件中。或者将标准输入中的数据写入目标文件。 </dst></localsrc></p><ul><li>hadoop fs -appendToFile - /dataAnalysis/duliming/output/stdinMergeFile.csv </li><li>hadoop fs -appendToFile /usr/local/usrJar/duliming/pcbin/pcbin_month/000002_0 /usr/local/usrJar/duliming/pcbin/pcbin_month/000003_0 /dataAnalysis/duliming/output/mergeFile.csv</li></ul><h3 id="chgrp"><a href="#chgrp" class="headerlink" title="[chgrp]"></a>[chgrp]</h3><p>grp [-R] GROUP URI [URI …] </p><ul><li>hadoop fs -chgrp hdfs /dataAnalysis/duliming/output/mergeFile.csv</li></ul><h3 id="chmod"><a href="#chmod" class="headerlink" title="[chmod]"></a>[chmod]</h3><p>使用方法：hadoop fs -chmod [-R] &lt;MODE[,MODE]… | OCTALMODE&gt; URI [URI …] 说明：修改文件权限。-R表示是否递归。修改者必须拥有该目录权限，或者是拥有者的父用户。 Example:<br>hadoop fs -chmod 777 /dataAnalysis/duliming/output/mergeFile.csv</p><h3 id="chown"><a href="#chown" class="headerlink" title="[chown]"></a>[chown]</h3><p>使用方法：hadoop fs -chown [-R] [OWNER][:[GROUP]] URI [URI ] 说明：修改文件拥有者。修改者必须拥有该文件或者是其父用户。-R表示递归。<br>Example: hadoop fs -chown hdfs /dataAnalysis/duliming/output/mergeFile.csv</p><h3 id="copyFromLocal"><a href="#copyFromLocal" class="headerlink" title="[copyFromLocal]"></a>[copyFromLocal]</h3><p>使用方法：hadoop fs -copyFromLocal <localsrc> URI 说明：拷贝本地文件到HDFS。类似于put命令，但可以拷贝目录。-f参数表示覆盖原来已存在目录。<br>Example: hadoop fs -copyFromLocal /usr/local/usrJar/duliming/DataAnalysis/output/AEP /dataAnalysis/duliming/output/</localsrc></p><h3 id="copyToLocal"><a href="#copyToLocal" class="headerlink" title="[copyToLocal]"></a>[copyToLocal]</h3><p>使用方法：hadoop fs -copyToLocal [-ignorecrc] [-crc] URI <localdst> 说明：拷贝HDFS文件到本地，类似于get命令，但可以拷贝目录。<br>Example: Hadoop fs –copyToLocal/dataAnalysis/duliming/output/ /usr/local/usrJar/duliming/DataAnalysis/output/AEP</localdst></p><h3 id="count"><a href="#count" class="headerlink" title="[count]"></a>[count]</h3><p>使用方法：hadoop fs -count [-q] [-h] [-v] <paths> 说明：统计目录下文件数，空间占用情况。 -h:输出格式化后的信息。 -v：输出表头<br>Example: hadoop fs -count -q -h -v /dataAnalysis/duliming/output<br><img alt="count" data-src="https://pic.superbed.cn/item/5d8d7210451253d178356356.png" class="lozad"></paths></p><h3 id="cp"><a href="#cp" class="headerlink" title="[cp]"></a>[cp]</h3><p>使用方法：hadoop fs -cp [-f] [-p | -p[topax]] URI [URI …] <dest><br>说明：将文件从源路径复制到目标路径。这个命令允许有多个源路径，此时目标路径必须是一个目录。 -f 如果目标 目录已存在，则覆盖之前的目录。<br>Example: hadoop fs -cp -f /dataAnalysis/duliming/output/wordcount/dataAnalysis/duliming/output/wordcount1</dest></p><h3 id="df"><a href="#df" class="headerlink" title="[df]"></a>[df]</h3><p>使用方法：hadoop fs -df [-h] URI [URI …] 显示目录空闲空间，-h：转换为更加易读的方式，比如67108864用64M代替。<br>Example: hadoop fs -df -h /dataAnalysis</p><h3 id="expunge"><a href="#expunge" class="headerlink" title="[expunge]"></a>[expunge]</h3><p>使用方法： hadoop fs –expunge<br>说明：清空回收站,hdfs默认是没有开启回收站功能的，需要配置中开启。</p><h3 id="get"><a href="#get" class="headerlink" title="[get]"></a>[get]</h3><p>使用方法：hadoop fs -get [-ignorecrc] [-crc] <src> <localdst><br>说明：复制文件到本地文件系统。可用-ignorecrc选项复制CRC校验失败的文件。使用-crc选项复制文件以及CRC信息。<br>Example: hadoop fs -get /user/hadoop/file localfile hadoop fs -get hdfs://host:port/user/hadoop/file localfile</localdst></src></p><h3 id="getmerge"><a href="#getmerge" class="headerlink" title="[getmerge]"></a>[getmerge]</h3><p>使用方法：hadoop fs -getmerge <src> <localdst> [addnl]<br>接受一个源目录和一个目标文件作为输入，并且将源目录中所有的文件连接成本地目标文件。addnl是可选的，用于指定在每个文件结尾添加一个换行符。</localdst></src></p><h3 id="ls"><a href="#ls" class="headerlink" title="[ls]"></a>[ls]</h3><p>使用方法：hadoop fs -ls <args><br>说明：如果是文件，则按照如下格式返回文件信息： 文件名 &lt;副本数&gt; 文件大小 修改日期 修改时间 权限 用户ID 组ID 如果是目录，则返回它直接子文件的一个列表，就像在Unix中一样。目录返回列表的信息如下： 目录名 </args></p><dir> 修改日期 修改时间 权限 用户ID 组ID<br>Example: hadoop fs -ls /user/hadoop/file1 /user/hadoop/file2 hdfs://host:port/user/hadoop/dir1 /nonexistentfile</dir><p></p><h3 id="lsr"><a href="#lsr" class="headerlink" title="[lsr]"></a>[lsr]</h3><p>使用方法：hadoop fs -lsr <args><br>说明：ls命令的递归版本。类似于Unix中的ls -R。</args></p><h3 id="mkdir"><a href="#mkdir" class="headerlink" title="[mkdir]"></a>[mkdir]</h3><p>使用方法：hadoop fs -mkdir <paths> 接受路径指定的uri作为参数，创建这些目录。其行为类似于Unix的mkdir -p，它会创建路径中的各级父目录。<br>Example: hadoop fs -mkdir /user/hadoop/dir1 /user/hadoop/dir2 hadoop fs -mkdir hdfs://host1:port1/user/hadoop/dir hdfs://host2:port2/user/hadoop/dir</paths></p><h3 id="mv"><a href="#mv" class="headerlink" title="[mv]"></a>[mv]</h3><p>使用方法： hadoop fs -mv URI [URI …] <dest><br>说明：将文件从源路径移动到目标路径。这个命令允许有多个源路径，此时目标路径必须是一个目录。不允许在不 同的文件系统间移动文件。<br>hadoop fs -mv /user/hadoop/file1 /user/hadoop/file2 hadoop fs -mv hdfs://host:port/file1 hdfs://host:port/file2 hdfs://host:port/file3 hdfs://host:port/dir1</dest></p><h3 id="put"><a href="#put" class="headerlink" title="[put]"></a>[put]</h3><p>使用方法：hadoop fs -put <localsrc> … <dst><br>说明：从本地文件系统中复制单个或多个源路径到目标文件系统。也支持从标准输入中读取输入写入目标文件系统。<br>Example: hadoop fs -put localfile /user/hadoop/hadoopfile hadoop fs -put localfile1 localfile2 /user/hadoop/hadoopdir hadoop fs -put - hdfs://host:port/hadoop/hadoopfile （从标准输入中读取输入。）</dst></localsrc></p><h3 id="rm"><a href="#rm" class="headerlink" title="[rm]"></a>[rm]</h3><p>使用方法： hadoop fs -rm URI [URI …]<br>删除指定的文件。只删除非空目录和文件。-r 递归删除。<br>Example: hadoop fs -rm hdfs://host:port/file /user/hadoop/emptydir </p><h3 id="setrep"><a href="#setrep" class="headerlink" title="[setrep]"></a>[setrep]</h3><p>使用方法： hadoop fs -setrep [-R] [-w] <numreplicas> <path><br>说明：改变一个文件的副本系数。-R选项用于递归改变目录下所有文件的副本系数。-w选项指定，改请求等待操作 执行结束。<br>hadoop fs -setrep -w 3 -R /user/hadoop/dir1</path></numreplicas></p><h3 id="stat"><a href="#stat" class="headerlink" title="[stat]"></a>[stat]</h3><p>使用方法： hadoop fs -tail [-f] URI<br>说明：返回指定路径的统计信息。<br>%F:文件<br>%b:文件大小<br>类型<br>%g:所属组<br>%o:block大小<br>%n:文件名<br>%r:复制因子数<br>%u:文件所有者<br>%Y,%y:修改日期<br>Example: hadoop fs -stat “%F %u:%g %b %y %n” /file</p><h3 id="tail"><a href="#tail" class="headerlink" title="[tail]"></a>[tail]</h3><p>使用方法： hadoop fs -tail [-f] URI 说明：将文件尾部1K字节的内容输出到stdout。支持-f选项，行为和Unix中一致。<br>Example: hadoop fs -tail pathname</p><h3 id="text"><a href="#text" class="headerlink" title="[text]"></a>[text]</h3><p>使用方法： hadoop fs -text <src> 说明：类似于cat。将源文件输出为文本格式。允许的格式是zip和TextRecordInputStream。</src></p><h3 id="touchz"><a href="#touchz" class="headerlink" title="[touchz]"></a>[touchz]</h3><p>使用方法：hadoop fs -touchz URI [URI …]<br>说明：创建一个0字节的空文件。<br>Example: Hadoop fs -touchz pathname</p><h3 id="truncate"><a href="#truncate" class="headerlink" title="[truncate]"></a>[truncate]</h3><p>使用方法：hadoop fs -truncate [-w] <length> <paths> 说明：文件截断，-w要求该命令等待恢复完成。<br>Example: hadoop fs -truncate 55 /user/hadoop/file1 /user/hadoop/file2 hadoop fs -truncate -w 127 hdfs://nn1.example.com/user/hadoop/file1</paths></length></p><h3 id="usage"><a href="#usage" class="headerlink" title="[usage]"></a>[usage]</h3><p>使用方法： hadoop fs -usage command<br>说明：返回命令的帮助信息。 </p><h3 id="find"><a href="#find" class="headerlink" title="[find]"></a>[find]</h3><p>使用方法：hadoop fs -find <path> … <expression> … 说明：查找满足表达式的文件和文件夹。没有配置path的话，默认的就是全部目录/；如果表达式没有配置，则默认 为-print。<br>-name pattern 不区分大小写，对大小写不敏感<br>-iname pattern 对大小写敏感。<br>-print 打印。 -print0 打印在一行。<br>Example: 。 Example:hadoop fs -find / -name test –print </expression></path></p><h3 id="getfacl"><a href="#getfacl" class="headerlink" title="[getfacl]"></a>[getfacl]</h3><p>使用方法：hadoop fs -getfacl [-R] <path><br>说明：获取文件的acl权限。-R指定递归查找<br>hadoop fs -getfacl -R /dir</path></p><h2 id="HDFS-dfsadmin"><a href="#HDFS-dfsadmin" class="headerlink" title="HDFS dfsadmin"></a>HDFS dfsadmin</h2><p>bin/hadoop dfsadmin命令支持一些和HDFS管理相关的操作。<br>bin/hadoop dfsadmin help 命令能列出所有当前支持的命令。<br>用法： hadoop dfsadmin [GENERIC_OPTIONS]<br>[-report]<br>[-safemode enter | leave | get | wait]<br>[-refreshNodes]<br>[-finalizeUpgrade]<br>[-upgradeProgress status | details | force]<br>[-metasave filename]<br>[-setQuota <quota> <dirname>…<dirname>]<br>[-clrQuota <dirname>…<dirname>]<br>[-help [cmd]]<br>说明：</dirname></dirname></dirname></dirname></quota></p><ul><li>-report: 报告文件系统的基本信息和统计信息。 </li><li>-safemode enter | leave | get | wait : 安全模式维护命令。安全模式是Namenode的一个状态，这种状态下，<br>Namenode </li></ul><ol><li>不接受对名字空间的更改(只读) </li><li>不复制或删除块<br>Namenode会在启动时自动进入安全模式，当配置的块最小百分比数满足最小的副本数条件时，会自动离开安全模式。安全模式可以手动进入，但是这样的话也必须手动关闭安全模式。</li></ol><ul><li>-refreshNodes :重新读取hosts和exclude文件，更新允许连到Namenode的或那些需要退出或入编的Datanode的集合。 </li><li>-finalizeUpgrade:终结HDFS的升级操作。Datanode删除前一个版本的工作目录，之后 Namenode也这样做。这个操作完结整个升级过程。 </li><li>-upgradeProgress status | details | force : 请求当前系统的升级状态，状态的细节，或者强制升级操作进行。</li><li>-metasave filename:保存Namenode的主要数据结构到hadoop.log.dir属性指定的目录下的<filename>文件。 对于下面的每一项，<filename>中都会一行内容与之对应<ol><li>Namenode收到的Datanode的心跳信号 </li><li>等待被复制的块 </li><li>正在被复制的块</li><li>等待被删除的块</li></ol></filename></filename></li><li>-setQuota <quota> <dirname>…<dirname>:为每个目录 <dirname>设定配额<quota>。目录配额是一个长整型整数，强制限定了目录树下的名字个数。 命令会在这个目录上工作良好，以下情况会报错： <ol><li>N不是一个正整数，或者 </li><li>用户不是管理员，或者 </li><li>这个目录不存在或是文件，或者</li><li>目录会马上超出新设定的配额。</li></ol></quota></dirname></dirname></dirname></quota></li><li>-clrQuota <dirname>…<dirname>:为每一个目录<dirname>清除配额设定。命令会在这个目录上工作良好，以下情况会报错： <ol><li>这个目录不存在或是文件，或者 </li><li>用户不是管理员。<br>如果目录原来没有配额不会报错。</li></ol></dirname></dirname></dirname></li></ul><p>Example:<br>hdfs –safemode enter|leave|get|wait<br>hdfs dfsadmin -getDatanodeInfo idh101:8010(<br>获取datanode信息，该命令用于检测datanode是否存活)<br>hdfs dfsadmin -shutdownDatanode idh101:8010 [upgrade]<br>hdfs dfsadmin -rollingUpgrade &lt;query|prepare|finalize&gt;<br>参照：<br>hadoop dfsadmin -setSpaceQuota 1g /user/seamon/<br>hadoop dfsadmin -clrSpaceQuota /user/seamon</p><p><a href="http://hadoop.apache.org/docs/r1.0.4/cn/commands_manual.html#dfsadmin" target="_blank" rel="noopener">http://hadoop.apache.org/docs/r1.0.4/cn/commands_manual.html#dfsadmin</a> <a href="http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html#dfsadmin" target="_blank" rel="noopener">http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html#dfsadmin</a><br><a href="http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HdfsRollingUpgrade.html" target="_blank" rel="noopener">http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HdfsRollingUpgrade.html</a></p><h2 id="HDFS-DistCp"><a href="#HDFS-DistCp" class="headerlink" title="HDFS DistCp"></a>HDFS DistCp</h2><p>DistCp（分布式拷贝）是用于大规模集群内部和集群之间拷贝的工具。 它使用 Map/Reduce实现文件分发，错误处理和恢复，以及报告生成。它把文件和目录的列表作为map任务的输入，每个任务会完成源列表中部分文件的拷贝。 由于使用了 Map/Reduce方法，这个工具在语义和执行上都会有特殊的地方。<br>使用方法： hadoop distcp hdfs://nn1:8020/foo/bar \ hdfs://nn2:8020/bar/foo<br>命令行中可以指定多个源目录：<br>hadoop distcp hdfs://nn1:8020/foo/a \<br>hdfs://nn1:8020/foo/b \<br>hdfs://nn2:8020/bar/foo<br>或者使用-f选项，从文件里获得多个源：<br>hadoop distcp -f hdfs://nn1:8020/srclist \<br>hdfs://nn2:8020/bar/foo<br>其中srclist 的内容是<br>hdfs://nn1:8020/foo/a<br>hdfs://nn1:8020/foo/b<br>参照：<a href="http://hadoop.apache.org/docs/r1.0.4/cn/distcp.html" target="_blank" rel="noopener">http://hadoop.apache.org/docs/r1.0.4/cn/distcp.html</a></p><h2 id="HDFS-getConf"><a href="#HDFS-getConf" class="headerlink" title="HDFS getConf"></a>HDFS getConf</h2><p>说明：用于获取hdfs配置信息。<br>Example:<br>hdfs getconf -namenodes<br>hdfs getconf -secondaryNameNodes<br>hdfs getconf -backupNodes<br>hdfs getconf -includeFile<br>hdfs getconf -excludeFile<br>hdfs getconf -nnRpcAddresses<br>hdfs getconf -confKey dfs.namenode.name.dir<br>hdfs getconf -confKey dfs.datanode.data.dir<br>hdfs getconf -confKey dfs.replication </p><p>参照：<a href="http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html#getconf" target="_blank" rel="noopener">http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html#getconf</a></p><h2 id="HDFS-oev"><a href="#HDFS-oev" class="headerlink" title="HDFS oev"></a>HDFS oev</h2><p>用法： hdfs oev [OPTIONS] -i INPUT_FILE -o OUTPUT_FILE<br>说明：命令hdfs oev用于查看edits文件。<br>-i,–inputFile <arg> 输入edits文件，如果是xml后缀，表示XML格式，其他表示二进制。 -o,–outputFile <arg> 输出文件，如果存在，则会覆盖。<br>可选参数：<br>-p,–processor <arg> 指定转换类型: binary (二进制格式), xml (默认，XML格式),stats (打印edits文件的静态统计信息)<br>-f,–fix-txids 重置输入edits文件中的transaction IDs<br>-r,–recover 使用recovery模式，跳过eidts中的错误记录。<br>-v,–verbose 打印处理时候的输出。<br>Example: hdfs oev -i /data1/hadoop/hdfs/name/current/edits_0000000000019382469-0000000000019383915 -o /home/hadoop/edits.xml<br>未指定-p选项，默认转换成xml格式，查看edits.xml文件。输出的xml文件中记录了文件路径（PATH），修改时间 （MTIME）、添加时间（ATIME）、客户端名称（CLIENT_NAME）、客户端地址（CLIENT_MACHINE）、权限 （PERMISSION_STATUS）等非常有用的信息。<br>当edits文件破损进而导致HDFS文件系统出现问题时，可以通过将原有的binary文件转换为xml文件，并手动编辑xml文件然后转回binary文件来实现。 </arg></arg></arg></p><p>参照： <a href="http://lxw1234.com/archives/2015/08/442.htm" target="_blank" rel="noopener">http://lxw1234.com/archives/2015/08/442.htm</a></p><h2 id="HDFS-oiv"><a href="#HDFS-oiv" class="headerlink" title="HDFS oiv"></a>HDFS oiv</h2><p>用法： hdfs oiv [OPTIONS] -i INPUT_FILE<br>说明：命令hdfs oiv用于将fsimage文件转换成其他格式的，如文本文件、XML文件。 -i,–inputFile <arg> 输入FSImage文件。<br>-o,–outputFile <arg> 输出转换后的文件，如果存在，则会覆盖。<br>可选参数：<br>-p,–processor <arg> 将FSImage文件转换成哪种格式：(Ls|XML|FileDistribution).默认为Ls.<br>-h,–help 显示帮助信息<br>Example:<br>hdfs oiv -i /data1/hadoop/dfs/name/current/fsimage_0000000000019372521 -o /home/hadoop/fsimage.txt<br>由于未指定-p选项，默认为Ls，出来的结果和执行hadoop fs –ls –R一样。<br>hdfs oiv -i /data1/hadoop/dfs/name/current/fsimage_0000000000019372521 -o /home/hadoop/fsimage.xml-p XML<br>指定-p XML，将fsimage文件转换成XML格式，查看fsimage.xml XML文件中包含了fsimage中的所有信息，比如inodeid、type、name、修改时间、权限、大小等等。</arg></arg></arg></p><p>参照：<a href="http://lxw1234.com/archives/2015/08/440.htm" target="_blank" rel="noopener">http://lxw1234.com/archives/2015/08/440.htm</a></p><h2 id="HDFS-fsck"><a href="#HDFS-fsck" class="headerlink" title="HDFS fsck"></a>HDFS fsck</h2><p>用法： hdfs fsck <path> [-list-corruptfileblocks |[-move | -delete | -openforwrite] [-files [-blocks [-locations | racks]]] [-includeSnapshots] [-storagepolicies] [-blockId <blk_id>] 说明：检查HDFS上文件和目录的健康状态、获取文件的block信息和位置信息等。 -list-corruptfileblocks:查看文件中损坏的块<br>-move:将损坏的文件移动至/lost+found目录<br>-delete:删除损坏的文件 -files:检查并列出所有文件状态 -openforwrite:检查并打印正在被打开执行写操作的文件 -blocks:打印文件的Block报告（需要和-files一起使用） -locations:打印文件块的位置信息（需要和-files -blocks一起使用。） -racks:打印文件块位置所在的机架信息<br>Example: hdfs fsck /hivedata/warehouse/liuxiaowen.db/lxw_product_names/ -list-corruptfileblocks<br>hdfs fsck /hivedata/warehouse/liuxiaowen.db/lxw_product_names/part-00168 –move hdfs fsck /hivedata/warehouse/liuxiaowen.db/lxw_product_names/part-00168 –delete<br>hdfs fsck /hivedata/warehouse/liuxiaowen.db/lxw_product_names/ -files </blk_id></path></p><p>参照：<a href="http://lxw1234.com/archives/2015/08/452.htm" target="_blank" rel="noopener">http://lxw1234.com/archives/2015/08/452.htm</a></p><h2 id="HDFS-balancer"><a href="#HDFS-balancer" class="headerlink" title="HDFS balancer"></a>HDFS balancer</h2><ul><li>添加新的DataNode节点。</li><li>人为干预，修改block副本数。</li><li>各个机器磁盘大小不一致。</li><li>长时间运行大量的delete操作等。</li></ul><p>Hadoop提供了Balancer工具来改善磁盘不均衡的问题。</p><p>用法： hdfs balancer [-threshold <threshold>] [-policy <policy>] [-exclude [-f <hosts-file> | <comma-separated list of hosts>]] [-include [-f <hosts-file> | <comma-separated list of hosts>]] [-idleiterations <idleiterations>]<br>说明：用于平衡hadoop集群中各datanode中的文件块分布，以避免出现部分datanode磁盘占用率高的问题。<br>-threshold <threshold>:表示的平衡的阀值，取值范围在0%到100%之间。每个Datanode中空间使用率与HDFS集群总 的空间使用率的差距百分比。<br>-policy <policy>：平衡策略，默认DataNode。应用于重新平衡HDFS存储的策略。默认DataNode策略平衡了DataNode级别的存储。这类似于之前发行版的平衡策略。BlockPool策略平衡了块池级别和DataNode级别的存储。 BlockPool策略仅适用于Federated HDFS服务。<br>-exclude/include:参数-exclude和-include是用来选择balancer时，可以指定哪几个DataNode之间重分布，也可以从HDFS 集群中排除哪几个节点不需要重分布<br>-idleiterations <iterations>:迭代检测的次数。<br>Example: hdfs balancer –threshold 10</iterations></policy></threshold></idleiterations></comma-separated></hosts-file></comma-separated></hosts-file></policy></threshold></p><h2 id="HDFS-快照"><a href="#HDFS-快照" class="headerlink" title="HDFS 快照"></a>HDFS 快照</h2><p>HDFS快照是一个只读的基于时间点文件系统拷贝。快照可以是整个文件系统的也可以是一部分。常用来作为数据备份，防止用户错误和容灾。<br>在datanode上面的blocks不会复制，做Snapshot的文件是纪录了block的列表和文件的大小，但是没有数据的复制Snapshot并不会影响HDFS的正常操作：修改会按照时间的反序记录，这样可以直接读取到最新的数据。快照数据是当前数据减去修改的部分计算出来的。<br>快照会存储在snapshottable的目录下。snapshottable下存储的snapshots最多为65535个。没有限制snapshottable目录 的数量。管理员可以设置任何的目录成为snapshottable。如果snapshottable里面存着快照，那么文件夹不能删除或者 改名。<br>快照常用操作<br>hdfs dfsadmin -allowSnapshot <path>：快照目录建立。如果这个操作成果，那么目录会变成snapshottable hdfs dfsadmin -disallowSnapshot <path>：文件夹里面的所有快照在失效快照前必须被删除，如果没有该目录会被建立。<br>hdfs dfsadmin -createSnapshot <path> [<snapshotname>]：snapshottable目录创建一个快照。这个操作需要 snapshottable目录的权限。<br>hdfs dfsadmin -deleteSnapshot <path> <snapshotname>：从一个snapshottable目录删除的快照。这个操作需要 snapshottable目录的权限。<br>hdfs dfsadmin -renameSnapshot <path> <oldname> <newname>：重命名快照。这个命令也需要snapshottable目录的权限。<br>hdfs lsSnapshottableDir：获取当前用户的所有snapshottable。 hdfs snapshotDiff <path> <fromsnapshot> <tosnapshot>：得到两个快照之间的不同。需要两个目录的权限。<br>参照：<a href="http://blog.csdn.net/linlinv3/article/details/44564313" target="_blank" rel="noopener">http://blog.csdn.net/linlinv3/article/details/44564313</a></tosnapshot></fromsnapshot></path></newname></oldname></path></snapshotname></path></snapshotname></path></path></path></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>HDFS架构及其实现原理。<ul><li>中心化设计，主从结构。NN+DN+SD+Client。</li><li>NN存储元数据信息，内存+持久化数据(fsimage+editlogs)</li><li>SD作为助手，定期合并editlogs到fsimage。</li></ul></li><li>HDFS写入/读取数据流程。<ul><li>建立流水线</li><li>写入数据</li><li>关闭流水线</li></ul></li><li>HDFS shell命令。</li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hadoop离线计算-1:初识Hadoop</title>
      <link href="/2019/09/27/hadoop-1/"/>
      <url>/2019/09/27/hadoop-1/</url>
      
        <content type="html"><![CDATA[<h1 id="初识Hadoop"><a href="#初识Hadoop" class="headerlink" title="初识Hadoop"></a>初识Hadoop</h1><h2 id="前言-数据存储与分析"><a href="#前言-数据存储与分析" class="headerlink" title="前言-数据存储与分析"></a>前言-数据存储与分析</h2><ul><li>硬件故障问题如何解决？<ul><li>RAID </li><li>HDFS<br>  HDFS建立的思想是：一次写入、多次读取模式是最高效的。 HDFS是一个高度容错性的系统，适合部署在廉价的机器上。 HDFS能提供高吞吐量的数据访问，非常适合大规模数据集上的应用</li></ul></li><li>数据打散分布到多台机器的多块硬盘上，如何保证数据的正确性和一致性？<ul><li>Mapreduce</li></ul></li></ul><h2 id="Hadoop生态圈"><a href="#Hadoop生态圈" class="headerlink" title="Hadoop生态圈"></a>Hadoop生态圈</h2><p><img alt="Hadoop生态圈" data-src="https://pic.superbed.cn/item/5d8c7bc2451253d178dcf470.png" class="lozad"></p><h2 id="Hadoop分布式文件系统"><a href="#Hadoop分布式文件系统" class="headerlink" title="Hadoop分布式文件系统"></a>Hadoop分布式文件系统</h2><h3 id="官方定义："><a href="#官方定义：" class="headerlink" title="官方定义："></a>官方定义：</h3><p>分布式存储系统HDFS（Hadoop Distributed File System）。<br>其实就是一个文件系统，类似于linux的文件系统。有目录，目录下可以存储文件。但它又是一个分布式的文件系统。</p><h3 id="基本原理："><a href="#基本原理：" class="headerlink" title="基本原理："></a>基本原理：</h3><ul><li>将文件切分成等大的数据块。分别存储到多台机器上。</li><li>每个数据块存在多个备份。</li><li>将数据切分、容错、负载均衡等功能透明化。</li><li>可将HDFS看成是一个巨大、具有容错性的磁盘。<br><img alt="HDFS架构" data-src="https://pic.superbed.cn/item/5d8c8f8d451253d178e227b1.png" class="lozad"></li></ul><h3 id="优点："><a href="#优点：" class="headerlink" title="优点："></a>优点：</h3><ul><li>处理超大文件</li><li>流式的数据访问</li><li>运行于廉价的商业机器集群上</li></ul><h3 id="缺点："><a href="#缺点：" class="headerlink" title="缺点："></a>缺点：</h3><ul><li>不适合存储大量小文件</li><li>不适合低延迟数据访问</li><li>不支持多用户写入及任意修改文件</li></ul><h2 id="Hadoop资源管理器-Yarn"><a href="#Hadoop资源管理器-Yarn" class="headerlink" title="Hadoop资源管理器-Yarn"></a>Hadoop资源管理器-Yarn</h2><ul><li>Hadoop2.0提出的资源管理层</li><li>用于集群资源管理和应用调度</li><li>使得多种计算框架可以运行在一个集群中</li><li>mapreduce仅仅是Yarn的一种应用模式</li></ul><h2 id="Hadoop资源管理器-架构"><a href="#Hadoop资源管理器-架构" class="headerlink" title="Hadoop资源管理器-架构"></a>Hadoop资源管理器-架构</h2><p><img alt="Hadoop资源管理器-架构" data-src="https://pic.superbed.cn/item/5d8d59af451253d1782fb829.png" class="lozad"></p><ul><li>资源管理器(Resource Manager, RM):每个集群中都有一个RM的守护进程，专门负责集群中可用资源的分配和管理。</li><li>节点管理器(Node Manager,NM):每个节点都有一个NM的守护进程，负责节点的本地资源管理。在RM中，NM代表本地节点。</li><li>Application Mater(AM):每个应用都有一个AM的守护进程，它封装了应用程序所有的逻辑结构和依赖库信息。AM负责与RM进行资源协商，并协同NM工作以完成应用的功能。</li><li>容器(Container):这是分配给具体应用的资源的抽象形式。AM是一个启动和管理应用 整个生命周期的特殊容器。</li><li>客户端(Client):这是集群中能向RM提交应用的实例，并且提供了执行应用所需的AM类型。</li></ul><h2 id="Hadoop资源管理器-调度器"><a href="#Hadoop资源管理器-调度器" class="headerlink" title="Hadoop资源管理器-调度器"></a>Hadoop资源管理器-调度器</h2><ul><li>FIFO Scheduler：FIFO Scheduler把应用按提交的顺序排成一个队列，这是一个先进先出队列，在进行资源分配的时候，先给队列中最头上的应用进行分配资源，待最头上的应用需求满足后再给下一个分配，以此类推。</li><li>Capacity Scheduler：而对于Capacity调度器，有一个专门的队列用来运行小任务，但是为小任务专门设置一个队列会预先占用一定的集群资源，这就导致大任务的执行时间会落后于使用FIFO调度器时的时间。</li><li>Fair Scheduler：在Fair调度器中，我们不需要预先占用一定的系统资源，Fair调度器会为所有运行的job动态的调整系统资源。当第一个大job提交时，只有这一个job在运行，此时它获得了所有集群资源；当第二个小任务提交后，Fair调度器会分配一半资源给这个小任务，让这两个任务公平的共享集群资源。</li></ul><h2 id="Hadoop分布式计算框架-MapRecuce"><a href="#Hadoop分布式计算框架-MapRecuce" class="headerlink" title="Hadoop分布式计算框架-MapRecuce"></a>Hadoop分布式计算框架-MapRecuce</h2><p><img alt="MapReduce" data-src="https://pic.superbed.cn/item/5d8c974b451253d178e41743.png" class="lozad"></p><h3 id="为什么需要？"><a href="#为什么需要？" class="headerlink" title="为什么需要？"></a>为什么需要？</h3><ul><li>并行计算技术和并行程序设计的复杂性</li><li>海量数据处理需要有效的并行处理技术</li><li>MapReduce是面向海量数据比较成功的技术</li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark内存计算-1:初识Spark</title>
      <link href="/2019/09/26/spark-1/"/>
      <url>/2019/09/26/spark-1/</url>
      
        <content type="html"><![CDATA[<h1 id="初识Spark"><a href="#初识Spark" class="headerlink" title="初识Spark"></a>初识Spark</h1><h2 id="大数据技术框架"><a href="#大数据技术框架" class="headerlink" title="大数据技术框架"></a>大数据技术框架</h2><p><img alt="大数据技术框架-1" data-src="https://pic.superbed.cn/item/5d8c7975451253d178dc6217.png" class="lozad"><br><img alt="大数据技术框架-2" data-src="https://pic.superbed.cn/item/5d8c7b5f451253d178dcdca7.png" class="lozad"><br><img alt="大数据技术框架-3" data-src="https://pic.superbed.cn/item/5d8c7bc2451253d178dcf470.png" class="lozad"></p><h2 id="Spark背景-HDFS"><a href="#Spark背景-HDFS" class="headerlink" title="Spark背景-HDFS"></a>Spark背景-HDFS</h2><h3 id="官方定义："><a href="#官方定义：" class="headerlink" title="官方定义："></a>官方定义：</h3><p>分布式存储系统HDFS（Hadoop Distributed File System）。<br>其实就是一个文件系统，类似于linux的文件系统。有目录，目录下可以存储文件。但它又是一个分布式的文件系统。</p><h3 id="基本原理："><a href="#基本原理：" class="headerlink" title="基本原理："></a>基本原理：</h3><ul><li>将文件切分成等大的数据块。分别存储到多台机器上。</li><li>每个数据块存在多个备份。</li><li>将数据切分、容错、负载均衡等功能透明化。</li><li>可将HDFS看成是一个巨大、具有容错性的磁盘。<br><img alt="HDFS架构" data-src="https://pic.superbed.cn/item/5d8c8f8d451253d178e227b1.png" class="lozad"></li></ul><h3 id="优点："><a href="#优点：" class="headerlink" title="优点："></a>优点：</h3><ul><li>处理超大文件</li><li>流式的数据访问</li><li>运行于廉价的商业机器集群上</li></ul><h3 id="缺点："><a href="#缺点：" class="headerlink" title="缺点："></a>缺点：</h3><ul><li>不适合存储大量小文件</li><li>不适合低延迟数据访问</li><li>不支持多用户写入及任意修改文件</li></ul><h2 id="Spark背景-资源调度"><a href="#Spark背景-资源调度" class="headerlink" title="Spark背景-资源调度"></a>Spark背景-资源调度</h2><ul><li>Hadoop2.0提出的资源管理层</li><li>用于集群资源管理和应用调度</li><li>使得多种计算框架可以运行在一个集群中</li><li>mapreduce仅仅是Yarn的一种应用模式</li></ul><h2 id="Spark背景-Mapreduce"><a href="#Spark背景-Mapreduce" class="headerlink" title="Spark背景-Mapreduce"></a>Spark背景-Mapreduce</h2><p>一个大数据若可以分为具有相同计算过程的数据块，并且这些数据块之间<strong>不存在数据依赖</strong>关系，则提高处理速度的最好方法就是<strong>并行计算</strong>。<br>Master：负责划分和分配任务<br>Worker：负责数据块计算</p><h3 id="为什么需要？"><a href="#为什么需要？" class="headerlink" title="为什么需要？"></a>为什么需要？</h3><ul><li>并行计算技术和并行程序设计的复杂性</li><li>海量数据处理需要有效的并行处理技术</li><li>MapReduce是面向海量数据比较成功的技术</li></ul><h3 id="大规模数据处理时，MapReduce在三个层次上的基本构思："><a href="#大规模数据处理时，MapReduce在三个层次上的基本构思：" class="headerlink" title="大规模数据处理时，MapReduce在三个层次上的基本构思："></a>大规模数据处理时，MapReduce在三个层次上的基本构思：</h3><p>1.如何对付大数据处理：分而治之<br>2.上升到抽象模型：Mapper与Reducer<br>3.上升到构架：统一构架，为程序员隐藏系统层细节</p><h3 id="结论："><a href="#结论：" class="headerlink" title="结论："></a>结论：</h3><p><strong>不可分拆的计算任务或者相互间有依赖关系的数据无法进行并行计算！</strong><br><img alt="MapReduce" data-src="https://pic.superbed.cn/item/5d8c974b451253d178e41743.png" class="lozad"></p><h3 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h3><pre class=" language-java"><code class="language-java"><span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">class</span> <span class="token class-name">Map</span> <span class="token keyword">extends</span> <span class="token class-name">Mapper</span><span class="token operator">&lt;</span>Object<span class="token punctuation">,</span>Text<span class="token punctuation">,</span>Text<span class="token punctuation">,</span>IntWritable<span class="token operator">></span><span class="token punctuation">{</span>    <span class="token comment" spellcheck="true">//one表示单词只出现一次</span>    <span class="token keyword">private</span> <span class="token keyword">static</span> IntWriteable one <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">IntWritable</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">//word存储切下的单词</span>    <span class="token keyword">private</span> Text word <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Text</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">map</span><span class="token punctuation">(</span>Object key<span class="token punctuation">,</span>Text value<span class="token punctuation">,</span>Context context<span class="token punctuation">)</span> <span class="token keyword">throw</span> IOException<span class="token punctuation">,</span>InterruptedException<span class="token punctuation">{</span>        <span class="token comment" spellcheck="true">//对输入的行切词</span>        StringTokenizer st <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">StringTokenizer</span><span class="token punctuation">(</span>value<span class="token punctuation">.</span><span class="token function">toString</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">while</span><span class="token punctuation">(</span>st<span class="token punctuation">.</span><span class="token function">hasMoreTokens</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>            word<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span>st<span class="token punctuation">.</span><span class="token function">nextToken</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//切下的单词存入word</span>            context<span class="token punctuation">.</span><span class="token function">write</span><span class="token punctuation">(</span>word<span class="token punctuation">,</span>one<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre><h3 id="Reduce"><a href="#Reduce" class="headerlink" title="Reduce"></a>Reduce</h3><pre class=" language-java"><code class="language-java"><span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">class</span> <span class="token class-name">Reduce</span> <span class="token keyword">extends</span> <span class="token class-name">Reducer</span><span class="token operator">&lt;</span>Text<span class="token punctuation">,</span>IntWritable<span class="token punctuation">,</span>Text<span class="token punctuation">,</span>IntWritable<span class="token operator">></span><span class="token punctuation">{</span>    <span class="token comment" spellcheck="true">//result记录单词的频数</span>    <span class="token keyword">private</span> <span class="token keyword">static</span> IntWritable result <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">IntWritable</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">reduce</span><span class="token punctuation">(</span>Text<span class="token operator">/</span>key<span class="token punctuation">,</span>Iterable<span class="token operator">&lt;</span>IntWritable<span class="token operator">></span> value<span class="token punctuation">,</span>Context context<span class="token punctuation">)</span> <span class="token keyword">throw</span> IOException<span class="token punctuation">,</span>InterruptedException<span class="token punctuation">{</span>        <span class="token keyword">int</span> sum <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>        <span class="token comment" spellcheck="true">//对获取的&lt;key,value-list>计算value的和</span>        <span class="token keyword">for</span><span class="token punctuation">(</span>IntWritable val<span class="token operator">:</span>values<span class="token punctuation">)</span><span class="token punctuation">{</span>            sum <span class="token operator">+=</span> val<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>        <span class="token comment" spellcheck="true">//将频数设置到result</span>        result<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span>sum<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token comment" spellcheck="true">//收集结果</span>        context<span class="token punctuation">.</span><span class="token function">write</span><span class="token punctuation">(</span>key<span class="token punctuation">,</span>result<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre><h3 id="局限性"><a href="#局限性" class="headerlink" title="局限性"></a>局限性</h3><ul><li>仅支持Map和Reduce操作（编程简单但是代码量大）</li><li>处理效率低<ul><li>Map中间结果写入磁盘。多个MapReduce之间通过HDFS交换数据</li><li>Map和Reduce端均需要排序</li><li>无法充分利用内存</li><li>任务调度和启用的开销较大</li></ul></li><li>不适合迭代计算（机器学习，图计算等）、交互式处理（数据挖掘）、流处理</li><li>编程不够灵活</li><li>不支持SQL语句，可以称其为分布式计算的编程语言</li></ul><h2 id="Spark背景：计算框架多样化"><a href="#Spark背景：计算框架多样化" class="headerlink" title="Spark背景：计算框架多样化"></a>Spark背景：计算框架多样化</h2><h4 id="现有的计算框架"><a href="#现有的计算框架" class="headerlink" title="现有的计算框架"></a>现有的计算框架</h4><ul><li>批处理：MapReduce、Hive、Spark…</li><li>流式计算：Storm</li><li>交互式计算：Impala、Presto</li><li>同时处理三种计算：<strong>Spark2.0</strong></li></ul><h2 id="Spark背景：什么是Spark"><a href="#Spark背景：什么是Spark" class="headerlink" title="Spark背景：什么是Spark"></a>Spark背景：什么是Spark</h2><h3 id="Spark："><a href="#Spark：" class="headerlink" title="Spark："></a>Spark：</h3><p>Spark是一个通用的、基于内存的<strong>并行计算</strong>框架。Spark也是基于<strong>MapReduce</strong>算法模式实现的分布式计算框架，并拥有Hadoop MapReduce的优点，解决了Hadoop MapReduce的诸多缺点。</p><h2 id="Spark生态系统："><a href="#Spark生态系统：" class="headerlink" title="Spark生态系统："></a>Spark生态系统：</h2><p>包含了Spark core、SparkSQL、Spark Streamimg、GraphX、MLlib、SparkR等子项目；<br>Spark支持Java、scala、Python、R、SQL语言；<br>Spark支持Standalone、Yarn、Mesos、K8s多种部署模式。<br><img alt="Spark生态系统" data-src="https://pic.superbed.cn/item/5d8cae10451253d178eb0712.png" class="lozad"></p><h3 id="Spark基本概念"><a href="#Spark基本概念" class="headerlink" title="Spark基本概念"></a>Spark基本概念</h3><h4 id="Spark-Core-Spark的核心功能实现"><a href="#Spark-Core-Spark的核心功能实现" class="headerlink" title="Spark Core : Spark的核心功能实现"></a>Spark Core : Spark的核心功能实现</h4><ul><li><strong>SparkContext</strong>： 通常而言，Driver Application的执行与输出都是通过SparkContext来完成的，在正式提交Application之前，首先需要初始化SparkContext。SparkContext内置的DAGScheduler负责创建Job,将DAG中的RDD划分到不同的stage,提交Stage等功能。内置的taskScheduler负责资源的申请、任务的提交及请求集群对任务的调度等工作。</li><li><strong>存储体系</strong>： Spark优先考虑使用各个节点的内存作为存储，当内存不足时才会考虑使用磁盘，使得Spark适用于实时计算、流式计算等场景。此外，Spark还提供了以内存为中心的高容错的分布式文件系统Alluxio供用户选择。Alluxio能够为Spark提供可靠的内存级文件共享服务。</li><li><strong>计算引擎</strong>： 计算引擎由SparkContext中的DAGScheduler、RDD以及具体节点上的Executor负责执行的Map和Reduce任务组成。DAGScheduler和RDD虽然位于SparkContext的内部，但是任务的正式提交与执行之前会将Job中的RDD组织成为有向无环图(建成DAG)，并对Stage进行划分，决定任务执行阶段的任务的数量、迭代计算、Shuffle等过程。</li><li><strong>部署模式</strong>： 由于单节点不足以提供足够的存储和计算能力，所以作为大数据处理的Spark在SparkContext的TaskScheduler的组件中提供了对Standalone部署模式的实现和在YARN、Mesos、k8s等分布式资源管理系统的支持。通过使用Standalone、YARN、Mesos等部署模式为Task分配计算资源，提供任务的并发执行效率。除了可用于生产环境的StandAlone、Yarn、Mesos、K8s等部署模式外，Spark还提供了Local模式和local-cluster模式便于我们开发和调试。</li></ul><h4 id="Spark-SQL："><a href="#Spark-SQL：" class="headerlink" title="Spark SQL："></a>Spark SQL：</h4><p>提供SQL的能力，便于熟悉关系型数据库操作的工程师进行交互查询。此外，还熟悉Hadoop的用户提供Hive SQL的处理能力。</p><h4 id="Spark-Streaming："><a href="#Spark-Streaming：" class="headerlink" title="Spark Streaming："></a>Spark Streaming：</h4><p>提供流式计算能力，支持Kafka、Flume、Twitter、MQTT、ZeroMQ、Kinesis和简单的TCP套接字等数据源。此外还提供窗口操作。</p><h4 id="GraphX："><a href="#GraphX：" class="headerlink" title="GraphX："></a>GraphX：</h4><p>提供图计算能力，支持分布式，Pregel提供的API可以解决图计算中的常见问题。</p><h4 id="MLlib："><a href="#MLlib：" class="headerlink" title="MLlib："></a>MLlib：</h4><p>提供机器学习相关的统计、分类、回归等领域的多种算法实现。其一致性的API接口大大降低了用户的学习成本。<br><strong>SparkSQL、Spark Streaming、GraphX、Mllib的能力都是建立在Spring Core之上。</strong></p><h2 id="计算类型及应用场景"><a href="#计算类型及应用场景" class="headerlink" title="计算类型及应用场景"></a>计算类型及应用场景</h2><ul><li>批处理计算：对时间没有严格要求，吞吐量高</li><li>迭代式与DAG计算：机器学习算法（DAG是有向无环图（Directed Acyclic Graph）的简称。在大数据处理中，DAG计算常常指的是将计算任务在内部分解成为若干个子任务，将这些子任务之间的逻辑关系或顺序构建成DAG（有向无环图）结构。）</li><li>交互式计算：支持类SQL语言，快速进行数据分析</li><li>流式计算：数据像流水一样流入进来，需实时对其处理分析</li></ul><h2 id="Spark特性-DAG"><a href="#Spark特性-DAG" class="headerlink" title="Spark特性-DAG"></a>Spark特性-DAG</h2><p><img alt="DAG" data-src="https://pic.superbed.cn/item/5d8cb4e9451253d178ed3cbe.png" class="lozad"></p><h2 id="Spark核心概念-RDD"><a href="#Spark核心概念-RDD" class="headerlink" title="Spark核心概念-RDD"></a>Spark核心概念-RDD</h2><h3 id="RDD：弹性分布式数据集-Resilient-Distributed-Datasets"><a href="#RDD：弹性分布式数据集-Resilient-Distributed-Datasets" class="headerlink" title="RDD：弹性分布式数据集(Resilient Distributed Datasets)"></a><u>RDD：弹性分布式数据集(Resilient Distributed Datasets)</u></h3><ul><li>RDD是Spark中计算和数据的抽象，它标识已经分片(partition),不可变的并能够被并行计算的数据集合。</li><li>RDD可以被存储在磁盘中也可以被存储到内存中。</li><li>RDD提供给我们很多方便的数据变换方法，这些变换操作分为两种类型（<strong>Transformation和Action</strong>）</li><li>RDD的生成方式有两种：数据源读入，其他RDD通过Transformation操作转换。</li><li>RDD失败后自动重构。<br><img alt="RDD组成" data-src="https://pic.superbed.cn/item/5d8cb67a451253d178edd238.png" class="lozad"></li><li>一个RDD由多个Partition构成(计算时一个partition对应一个task)。</li><li>每个Partion数据可以存储在内存中也可以存储在磁盘中(用户可控制)。</li><li>RDD的操作分为Trasformation和Action两种操作。</li></ul><h2 id="Spark核心概念-RDD基本操作"><a href="#Spark核心概念-RDD基本操作" class="headerlink" title="Spark核心概念-RDD基本操作"></a>Spark核心概念-RDD基本操作</h2><p><img alt="RDD基本操作" data-src="https://pic.superbed.cn/item/5d8cb8c1451253d178ee8918.png" class="lozad"></p><h3 id="Transformation："><a href="#Transformation：" class="headerlink" title="Transformation："></a>Transformation：</h3><p>把一个RDD转换为另外一个RDD。例如：map,filter,groupBy,reduceBy等。</p><h3 id="Action："><a href="#Action：" class="headerlink" title="Action："></a>Action：</h3><p>通过RDD计算得到一个或者一组值。例如：count</p><h3 id="WordCount-example"><a href="#WordCount-example" class="headerlink" title="WordCount example"></a>WordCount example</h3><p><img alt="WordCount" data-src="https://pic.superbed.cn/item/5d8cba22451253d178eefb7c.png" class="lozad"><br><strong>WordCount.java</strong></p><pre class=" language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>neu<span class="token punctuation">.</span>mapreduce<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>io<span class="token punctuation">.</span>IOException<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>StringTokenizer<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>conf<span class="token punctuation">.</span>Configuration<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>fs<span class="token punctuation">.</span>Path<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span>IntWritable<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span>LongWritable<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span>Text<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span>Job<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span>Mapper<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span>Reducer<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span>lib<span class="token punctuation">.</span>input<span class="token punctuation">.</span>FileInputFormat<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span>lib<span class="token punctuation">.</span>output<span class="token punctuation">.</span>FileOutputFormat<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span>lib<span class="token punctuation">.</span>partition<span class="token punctuation">.</span>HashPartitioner<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>util<span class="token punctuation">.</span>GenericOptionsParser<span class="token punctuation">;</span><span class="token keyword">import</span> com<span class="token punctuation">.</span>neu<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span>util<span class="token punctuation">.</span>HDFSUtils<span class="token punctuation">;</span><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">WordCount</span> <span class="token punctuation">{</span>    <span class="token comment" spellcheck="true">//继承mapper接口，设置map的输入类型为&lt;Object,Text></span>    <span class="token comment" spellcheck="true">//输出类型为&lt;Text,IntWritable></span>    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">class</span> <span class="token class-name">Map</span> <span class="token keyword">extends</span> <span class="token class-name">Mapper</span><span class="token operator">&lt;</span>LongWritable<span class="token punctuation">,</span>Text<span class="token punctuation">,</span>Text<span class="token punctuation">,</span>IntWritable<span class="token operator">></span><span class="token punctuation">{</span>        <span class="token comment" spellcheck="true">//one表示单词出现一次</span>        <span class="token keyword">private</span> <span class="token keyword">static</span> IntWritable one <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">IntWritable</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token comment" spellcheck="true">//word存储切下的单词</span>        <span class="token keyword">private</span> Text word <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Text</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">map</span><span class="token punctuation">(</span>LongWritable key<span class="token punctuation">,</span>Text value<span class="token punctuation">,</span>Context context<span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span>InterruptedException<span class="token punctuation">{</span>            <span class="token comment" spellcheck="true">//对输入的行切词</span>            StringTokenizer st <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">StringTokenizer</span><span class="token punctuation">(</span>value<span class="token punctuation">.</span><span class="token function">toString</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token keyword">while</span><span class="token punctuation">(</span>st<span class="token punctuation">.</span><span class="token function">hasMoreTokens</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{</span>                word<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span>st<span class="token punctuation">.</span><span class="token function">nextToken</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//切下的单词存入word</span>                context<span class="token punctuation">.</span><span class="token function">write</span><span class="token punctuation">(</span>word<span class="token punctuation">,</span> one<span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">//继承reducer接口，设置reduce的输入类型&lt;Text,IntWritable></span>    <span class="token comment" spellcheck="true">//输出类型为&lt;Text,IntWritable></span>    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">class</span> <span class="token class-name">Reduce</span> <span class="token keyword">extends</span> <span class="token class-name">Reducer</span><span class="token operator">&lt;</span>Text<span class="token punctuation">,</span>IntWritable<span class="token punctuation">,</span>Text<span class="token punctuation">,</span>IntWritable<span class="token operator">></span><span class="token punctuation">{</span>        <span class="token comment" spellcheck="true">//result记录单词的频数</span>        <span class="token keyword">private</span> <span class="token keyword">static</span> IntWritable result <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">IntWritable</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">reduce</span><span class="token punctuation">(</span>Text key<span class="token punctuation">,</span>Iterable<span class="token operator">&lt;</span>IntWritable<span class="token operator">></span> values<span class="token punctuation">,</span>Context context<span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span>InterruptedException<span class="token punctuation">{</span>            <span class="token keyword">int</span> sum <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>            <span class="token comment" spellcheck="true">//对获取的&lt;key,value-list>计算value的和</span>            <span class="token keyword">for</span><span class="token punctuation">(</span>IntWritable val<span class="token operator">:</span>values<span class="token punctuation">)</span><span class="token punctuation">{</span>                sum <span class="token operator">+=</span> val<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span>            <span class="token comment" spellcheck="true">//将频数设置到result</span>            result<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span>sum<span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token comment" spellcheck="true">//收集结果</span>            context<span class="token punctuation">.</span><span class="token function">write</span><span class="token punctuation">(</span>key<span class="token punctuation">,</span> result<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span>    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">class</span> <span class="token class-name">WordPartitioner</span> <span class="token keyword">extends</span> <span class="token class-name">HashPartitioner</span><span class="token operator">&lt;</span>Text<span class="token punctuation">,</span> IntWritable<span class="token operator">></span> <span class="token punctuation">{</span>        <span class="token annotation punctuation">@Override</span>        <span class="token keyword">public</span> <span class="token keyword">int</span> <span class="token function">getPartition</span><span class="token punctuation">(</span>Text key<span class="token punctuation">,</span> IntWritable value<span class="token punctuation">,</span> <span class="token keyword">int</span> numReduceTasks<span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token keyword">if</span><span class="token punctuation">(</span>key<span class="token punctuation">.</span><span class="token function">equals</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Text</span><span class="token punctuation">(</span><span class="token string">"Bootstrap"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>                <span class="token keyword">return</span> <span class="token number">2</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span>            <span class="token keyword">return</span> <span class="token keyword">super</span><span class="token punctuation">.</span><span class="token function">getPartition</span><span class="token punctuation">(</span>key<span class="token punctuation">,</span> value<span class="token punctuation">,</span> numReduceTasks <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">/**     * @param args     */</span>    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span>String<span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception<span class="token punctuation">{</span>        Configuration conf <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token comment" spellcheck="true">//conf.set("mapreduce.output.fileoutputformat.compress", "true");</span>        <span class="token comment" spellcheck="true">//conf.set("mapreduce.job.queuename ", "spark");</span>        <span class="token comment" spellcheck="true">//检查运行命令</span>        String<span class="token punctuation">[</span><span class="token punctuation">]</span> otherArgs <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">GenericOptionsParser</span><span class="token punctuation">(</span>conf<span class="token punctuation">,</span>args<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">getRemainingArgs</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">if</span><span class="token punctuation">(</span>otherArgs<span class="token punctuation">.</span>length <span class="token operator">!=</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">{</span>            System<span class="token punctuation">.</span>err<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"Usage WordCount &lt;int> &lt;out>"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            System<span class="token punctuation">.</span><span class="token function">exit</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>        HDFSUtils hdfs <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">HDFSUtils</span><span class="token punctuation">(</span>conf<span class="token punctuation">)</span><span class="token punctuation">;</span>        hdfs<span class="token punctuation">.</span><span class="token function">deleteDir</span><span class="token punctuation">(</span>args<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token comment" spellcheck="true">//配置作业名</span>        Job job <span class="token operator">=</span> Job<span class="token punctuation">.</span><span class="token function">getInstance</span><span class="token punctuation">(</span>conf<span class="token punctuation">,</span> <span class="token string">"word count"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token comment" spellcheck="true">//配置作业各个类</span>        job<span class="token punctuation">.</span><span class="token function">setJarByClass</span><span class="token punctuation">(</span>WordCount<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        job<span class="token punctuation">.</span><span class="token function">setMapperClass</span><span class="token punctuation">(</span>Map<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        job<span class="token punctuation">.</span><span class="token function">setMapOutputKeyClass</span><span class="token punctuation">(</span>Text<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        job<span class="token punctuation">.</span><span class="token function">setMapOutputValueClass</span><span class="token punctuation">(</span>IntWritable<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//      job.setCombinerClass(Reduce.class);</span>        job<span class="token punctuation">.</span><span class="token function">setReducerClass</span><span class="token punctuation">(</span>Reduce<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        job<span class="token punctuation">.</span><span class="token function">setOutputKeyClass</span><span class="token punctuation">(</span>Text<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        job<span class="token punctuation">.</span><span class="token function">setOutputValueClass</span><span class="token punctuation">(</span>IntWritable<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        job<span class="token punctuation">.</span><span class="token function">setNumReduceTasks</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//      job.setPartitionerClass(WordPartitioner.class);</span>        FileInputFormat<span class="token punctuation">.</span><span class="token function">addInputPath</span><span class="token punctuation">(</span>job<span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span>args<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//      FileInputFormat.setMaxInputSplitSize(job, 10);</span><span class="token comment" spellcheck="true">//      FileInputFormat.setMinInputSplitSize(job, 10);</span>        FileOutputFormat<span class="token punctuation">.</span><span class="token function">setOutputPath</span><span class="token punctuation">(</span>job<span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span>args<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//      FileOutputFormat.setCompressOutput(job, true);</span><span class="token comment" spellcheck="true">//      FileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class);</span>        System<span class="token punctuation">.</span><span class="token function">exit</span><span class="token punctuation">(</span>job<span class="token punctuation">.</span><span class="token function">waitForCompletion</span><span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">)</span> <span class="token operator">?</span> <span class="token number">0</span> <span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre><p><strong>WordCount_Spark.java</strong></p><pre class=" language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>neu<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>ArrayList<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>Iterator<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>List<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>Map<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>log4j<span class="token punctuation">.</span>Logger<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>spark<span class="token punctuation">.</span>SparkConf<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>spark<span class="token punctuation">.</span>SparkContext<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>spark<span class="token punctuation">.</span>api<span class="token punctuation">.</span>java<span class="token punctuation">.</span>JavaPairRDD<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>spark<span class="token punctuation">.</span>api<span class="token punctuation">.</span>java<span class="token punctuation">.</span>JavaRDD<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>spark<span class="token punctuation">.</span>api<span class="token punctuation">.</span>java<span class="token punctuation">.</span>JavaSparkContext<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>spark<span class="token punctuation">.</span>api<span class="token punctuation">.</span>java<span class="token punctuation">.</span>function<span class="token punctuation">.</span>FlatMapFunction<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>spark<span class="token punctuation">.</span>api<span class="token punctuation">.</span>java<span class="token punctuation">.</span>function<span class="token punctuation">.</span>Function2<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>spark<span class="token punctuation">.</span>api<span class="token punctuation">.</span>java<span class="token punctuation">.</span>function<span class="token punctuation">.</span>PairFunction<span class="token punctuation">;</span><span class="token keyword">import</span> scala<span class="token punctuation">.</span>Tuple2<span class="token punctuation">;</span><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">WordCount</span> <span class="token punctuation">{</span>    <span class="token keyword">private</span> <span class="token keyword">static</span> Logger log <span class="token operator">=</span> Logger<span class="token punctuation">.</span><span class="token function">getLogger</span><span class="token punctuation">(</span>WordCount<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token annotation punctuation">@SuppressWarnings</span><span class="token punctuation">(</span><span class="token string">"resource"</span><span class="token punctuation">)</span>    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span>String<span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token punctuation">{</span>        log<span class="token punctuation">.</span><span class="token function">info</span><span class="token punctuation">(</span><span class="token string">"开始执行wordcount!"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">try</span> <span class="token punctuation">{</span>            SparkConf conf <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">SparkConf</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            conf<span class="token punctuation">.</span><span class="token function">setAppName</span><span class="token punctuation">(</span><span class="token string">"wordCount"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            conf<span class="token punctuation">.</span><span class="token function">setMaster</span><span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            SparkContext sc <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">SparkContext</span><span class="token punctuation">(</span>conf<span class="token punctuation">)</span><span class="token punctuation">;</span>            JavaSparkContext jsc <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">JavaSparkContext</span><span class="token punctuation">(</span>sc<span class="token punctuation">)</span><span class="token punctuation">;</span>            log<span class="token punctuation">.</span><span class="token function">info</span><span class="token punctuation">(</span><span class="token string">"开始读取数据!"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            JavaRDD<span class="token operator">&lt;</span>String<span class="token operator">></span> data <span class="token operator">=</span> jsc<span class="token punctuation">.</span><span class="token function">textFile</span><span class="token punctuation">(</span><span class="token string">"file:///D:\\test.txt"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            log<span class="token punctuation">.</span><span class="token function">info</span><span class="token punctuation">(</span><span class="token string">"结束读取数据"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            JavaRDD<span class="token operator">&lt;</span>String<span class="token operator">></span> wordRDD <span class="token operator">=</span> data<span class="token punctuation">.</span><span class="token function">flatMap</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">FlatMapFunction</span><span class="token operator">&lt;</span>String<span class="token punctuation">,</span> String<span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>                <span class="token keyword">private</span> <span class="token keyword">static</span> <span class="token keyword">final</span> <span class="token keyword">long</span> serialVersionUID <span class="token operator">=</span> <span class="token operator">-</span>294828459740757725L<span class="token punctuation">;</span>                <span class="token keyword">public</span> Iterator<span class="token operator">&lt;</span>String<span class="token operator">></span> <span class="token function">call</span><span class="token punctuation">(</span>String t<span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token punctuation">{</span>                    String<span class="token punctuation">[</span><span class="token punctuation">]</span> words <span class="token operator">=</span> t<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                    List<span class="token operator">&lt;</span>String<span class="token operator">></span> list <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">ArrayList</span><span class="token operator">&lt;</span>String<span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                    <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> words<span class="token punctuation">.</span>length<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>                        list<span class="token punctuation">.</span><span class="token function">add</span><span class="token punctuation">(</span>words<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                    <span class="token punctuation">}</span>                    <span class="token keyword">return</span> list<span class="token punctuation">.</span><span class="token function">iterator</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                <span class="token punctuation">}</span>            <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            JavaPairRDD<span class="token operator">&lt;</span>String<span class="token punctuation">,</span> Integer<span class="token operator">></span> wordPairRDD <span class="token operator">=</span> wordRDD<span class="token punctuation">.</span><span class="token function">mapToPair</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">PairFunction</span><span class="token operator">&lt;</span>String<span class="token punctuation">,</span> String<span class="token punctuation">,</span> Integer<span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>                <span class="token keyword">private</span> <span class="token keyword">static</span> <span class="token keyword">final</span> <span class="token keyword">long</span> serialVersionUID <span class="token operator">=</span> <span class="token operator">-</span>3940041647950913972L<span class="token punctuation">;</span>                <span class="token keyword">public</span> Tuple2<span class="token operator">&lt;</span>String<span class="token punctuation">,</span> Integer<span class="token operator">></span> <span class="token function">call</span><span class="token punctuation">(</span>String t<span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token punctuation">{</span>                    <span class="token keyword">return</span> <span class="token keyword">new</span> <span class="token class-name">Tuple2</span><span class="token operator">&lt;</span>String<span class="token punctuation">,</span> Integer<span class="token operator">></span><span class="token punctuation">(</span>t<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                <span class="token punctuation">}</span>            <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            JavaPairRDD<span class="token operator">&lt;</span>String<span class="token punctuation">,</span> Integer<span class="token operator">></span> wordCountRS <span class="token operator">=</span> wordPairRDD<span class="token punctuation">.</span><span class="token function">reduceByKey</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Function2</span><span class="token operator">&lt;</span>Integer<span class="token punctuation">,</span> Integer<span class="token punctuation">,</span> Integer<span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>                <span class="token keyword">private</span> <span class="token keyword">static</span> <span class="token keyword">final</span> <span class="token keyword">long</span> serialVersionUID <span class="token operator">=</span> 1254368307373180066L<span class="token punctuation">;</span>                <span class="token keyword">public</span> Integer <span class="token function">call</span><span class="token punctuation">(</span>Integer v1<span class="token punctuation">,</span> Integer v2<span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token punctuation">{</span>                    <span class="token keyword">return</span> v1<span class="token punctuation">.</span><span class="token function">intValue</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> v2<span class="token punctuation">.</span><span class="token function">intValue</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                <span class="token punctuation">}</span>            <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            Map<span class="token operator">&lt;</span>String<span class="token punctuation">,</span> Integer<span class="token operator">></span> result <span class="token operator">=</span> wordCountRS<span class="token punctuation">.</span><span class="token function">collectAsMap</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>result<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>data<span class="token punctuation">.</span><span class="token function">count</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            log<span class="token punctuation">.</span><span class="token function">info</span><span class="token punctuation">(</span><span class="token string">"数据条数："</span> <span class="token operator">+</span> data<span class="token punctuation">.</span><span class="token function">count</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            log<span class="token punctuation">.</span><span class="token function">info</span><span class="token punctuation">(</span><span class="token string">"开始输出文件:D:\\output"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            wordCountRS<span class="token punctuation">.</span><span class="token function">saveAsTextFile</span><span class="token punctuation">(</span><span class="token string">"D:\\output"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            log<span class="token punctuation">.</span><span class="token function">info</span><span class="token punctuation">(</span><span class="token string">"结束输出文件D:\\output"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            log<span class="token punctuation">.</span><span class="token function">info</span><span class="token punctuation">(</span><span class="token string">"结束执行wordcount程序！"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span> <span class="token keyword">catch</span> <span class="token punctuation">(</span><span class="token class-name">Exception</span> e<span class="token punctuation">)</span> <span class="token punctuation">{</span>            log<span class="token punctuation">.</span><span class="token function">error</span><span class="token punctuation">(</span><span class="token string">"执行失败"</span><span class="token punctuation">,</span>e<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre><h2 id="Spark核心概念-RDD的缓存位置"><a href="#Spark核心概念-RDD的缓存位置" class="headerlink" title="Spark核心概念-RDD的缓存位置"></a>Spark核心概念-RDD的缓存位置</h2><p>RDD既可以存储为内存中也可以存储在磁盘里面。Spark为我们提供了一下几种方案。</p><ul><li>MEMORY_ONLY:将RDD 作为反序列化的的对象存储JVM 中。如果RDD不能被内存装下，一些分区将不会被缓存，并且在需要的时候被重新计算。<br>这是是默认的级别</li><li>MEMORY_AND_DISK:将RDD 作为反序列化的的对象存储在JVM 中。如果RDD不能被与内存装下，超出的分区将被保存在硬盘上，并且在需要时被读取</li><li>MEMORY_ONLY_SER:将RDD 作为序列化的的对象进行存储（每一分区占用一个字节数组）。<br>通常来说，这比将对象反序列化的空间利用率更高，尤其当使用fast serializer,但在读取时会比较占用CPU</li><li>MEMORY_AND_DISK_SER:与MEMORY_ONLY_SER 相似，但是把超出内存的分区将存储在硬盘上而不是在每次需要的时候重新计算</li><li>DISK_ONLY:只将RDD 分区存储在硬盘上</li><li>MEMORY_ONLY_2:与上述的存储级别一样，但是将每一个分区都被复制两份存储在集群结点上</li></ul><p>用户可以根据RDD提供的方法选择缓存方案:</p><pre class=" language-java"><code class="language-java">JavaRDD<span class="token operator">&lt;</span>String<span class="token operator">></span> data <span class="token operator">=</span> jsc<span class="token punctuation">.</span> <span class="token function">textFile</span><span class="token punctuation">(</span><span class="token string">"/data/external/dataCompletion/2017-05-11/cp_86.1000.11_20170605. csv"</span><span class="token punctuation">)</span><span class="token punctuation">;</span> data<span class="token punctuation">.</span> <span class="token function">cache</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span> data<span class="token punctuation">.</span> <span class="token function">persist</span><span class="token punctuation">(</span>StorageLeve1<span class="token punctuation">.</span> <span class="token function">MEMORY_AND_DISK</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>val NONE<span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Storagelevel</span><span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span>val DISK_ONLY <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">StorageLevel</span><span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span>val DISK_ONLY_2<span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Storagelevel</span><span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>val MEMORY_ONLY <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Storagelevel</span><span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">)</span>val MEMORY_ONLY_2<span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Storagelevel</span><span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>val MEMORY_ONLY_SER <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Storagelevel</span><span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span>val MEMORY_ONLY _SER_2<span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Storagelevel</span><span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>val MEMORY_AND_DISK <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Storagelevel</span><span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">)</span>val MEMORY_AND_DISK_2<span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Storagelevel</span><span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>val MEMORY_AND_DISK_SER <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">StorageLevel</span><span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span>val MEMORY_AND_DISK_SER_2<span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Storagelevel</span><span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>val OFF_HEAP <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Storagelevel</span><span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span></code></pre><h2 id="Spark基本架构"><a href="#Spark基本架构" class="headerlink" title="Spark基本架构"></a>Spark基本架构</h2><p><img alt="Spark基本架构" data-src="https://pic.superbed.cn/item/5d8cbc4a451253d178efa11e.png" class="lozad"></p><h3 id="Cluster-Manager："><a href="#Cluster-Manager：" class="headerlink" title="Cluster Manager："></a>Cluster Manager：</h3><p>Spark集群管理器，主要负责资源的分配和管理。集群管理器分配的资源属于一级分配，它将各个Worker上的内存、CPU等资源分配给应用程序，但是并不负责Executor的资源分配。目前，Standalone、Yarn、Mesos、EC2等都可作为Spark的集群管理器。</p><h3 id="Driver-Programe："><a href="#Driver-Programe：" class="headerlink" title="Driver Programe："></a>Driver Programe：</h3><p>客户端驱动程序，也可理解为客户端应用程序，用于将任务程序转换为RDD和DAG，并与Cluster Manager进行通信与调度。</p><h3 id="Worker-Node："><a href="#Worker-Node：" class="headerlink" title="Worker Node："></a>Worker Node：</h3><p>Spark的工作节点。对于Spark应用程序来说，由集群管理器分配得到的资源的Work节点主要负责以下工作：创建Executor，将资源和任务进一步分配给Executor，同步资源信息给Cluster Manager。</p><h3 id="Executor"><a href="#Executor" class="headerlink" title="Executor:"></a>Executor:</h3><p>执行计算任务的一线进程。主要负责任务的执行以及Worker、Driver Program的信息同步。每个应用程序都有自己的executor。每个excutor包含多个task。</p><h2 id="Spark运行模式"><a href="#Spark运行模式" class="headerlink" title="Spark运行模式"></a>Spark运行模式</h2><h3 id="Local模式："><a href="#Local模式：" class="headerlink" title="Local模式："></a>Local模式：</h3><p>单机运行，通常用于测试。</p><h3 id="Standalone模式："><a href="#Standalone模式：" class="headerlink" title="Standalone模式："></a>Standalone模式：</h3><p>独立运行在一个spark的集群中。</p><h3 id="Spark-On-Yarn-Mesos模式："><a href="#Spark-On-Yarn-Mesos模式：" class="headerlink" title="Spark On Yarn/Mesos模式："></a>Spark On Yarn/Mesos模式：</h3><p>Spark程序运行在资源管理器上，例如YARN/Mesos<br>Spark on Yarn存在两种模式：<br>    1.yarn-client <img alt="Client" data-src="https://pic.superbed.cn/item/5d8cbe53451253d178f02f67.png" class="lozad"><br>    2.yarn-cluster <img alt="Cluster" data-src="https://pic.superbed.cn/item/5d8cbe4f451253d178f02de6.png" class="lozad"><br><img alt="Spark运行模式" data-src="https://pic.superbed.cn/item/5d8cbd69451253d178eff7fa.png" class="lozad"></p><ul><li>Application Master:在YARN中，每个Application实例都有一个Application Master进程，它是Application启动的第一个容器。它负责和ResourceManager打交道，并请求资源。获取资源之后告诉NodeManager为其启动container。</li><li>一个Application实例对应一个Application Master进程，也是运行在一个YARN Container中，并且是第一个Container。</li><li>Application Master只负责从Resource Manager处获取资源。由Client与各个Container进行通信确认如何工作。</li></ul><p><img alt="Spark运行模式-1" data-src="https://pic.superbed.cn/item/5d8cbdd3451253d178f01157.png" class="lozad"><br>yarn-cluster和yarn-client模式的区别其实就是Application Master进程的区别，yarn-cluster模式下，driver运行在AM(Application Master)中，它负责向YARN申请资源，并监督作业的运行状况。当用户提交了作业之后，就可以关掉Client，作业会继续在YARN上运行。然而yarn-cluster模式不适合运行交互类型的作业。而yarn-client模式下，Application Master仅仅向YARN请求executor，client会和请求的container通信来调度他们工作，也就是说Client不能离开。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>软件需求分析设计-2</title>
      <link href="/2019/09/25/software-requirement-2/"/>
      <url>/2019/09/25/software-requirement-2/</url>
      
        <content type="html"><![CDATA[<h1 id="软件需求分析设计"><a href="#软件需求分析设计" class="headerlink" title="软件需求分析设计"></a>软件需求分析设计</h1><h2 id="第二章"><a href="#第二章" class="headerlink" title="第二章"></a>第二章</h2><h3 id="需求调研-获取："><a href="#需求调研-获取：" class="headerlink" title="需求调研/获取："></a>需求调研/获取：</h3><p>需求获取的重要性：最关键、最困难、最易出错、最需要交流<br>需求获取的前提条件： 领域知识（业务知识）、沟通能力</p><h3 id="需求：是产品必须完成的事（功能性需求）以及必须具备的品质"><a href="#需求：是产品必须完成的事（功能性需求）以及必须具备的品质" class="headerlink" title="需求：是产品必须完成的事（功能性需求）以及必须具备的品质"></a>需求：是产品必须完成的事（功能性需求）以及必须具备的品质</h3><p>需要的东西：需求形式（样式风格）、数据信息、需求的内容</p><p>需求的种类：<br>    + 功能性需求：系统应该做的<br>    + 非功能性需求：质量+约束，系统特定的特性或约束</p><p>需求层次：<br>    + 业务需求<br>        + 反映了组织结构或客户队系统产品高层次的目标要求<br>        + 业务目标：系统建设目标<br>    + 用户需求<br>        + 描述了用户使用产品必须完成的任务<br>    + 功能需求（软件需求）<br>        + 定义开发人员必须实现的软件功能，使得用户能完成他们的任务，从而满足业务需求<br>    + 技术需求（具体的实现细节）</p><p>需求分析极致目的：所有人做的都一样</p><h3 id="项目中标："><a href="#项目中标：" class="headerlink" title="项目中标："></a>项目中标：</h3><p>中标前：项目应标成功<br>    + 获取的要素：业务需求和用户需求 (全面性)<br>中标后：确定项目范围<br>    + 要点：全部用户需求、部分功能需求</p><h3 id="需求获取："><a href="#需求获取：" class="headerlink" title="需求获取："></a>需求获取：</h3><p>需求获取定义：<br>是通过客户、系统用户和其他与系统开发相关的人员交流发现系统需求的过程，是一个确定和理解不同用户种类的需要和限制的过程<br>需求获取总原则：<br>    + 先获取系统的总体目标（宏观）（业务需求）<br>    + 接着获取当前工作以及当前问题的信息（中观）（用户需求）<br>    + 然后系统应先处理的详细问题（微观）（用户需求）<br>需求获取第一步：<br>相关人员分析：是指那些直接或间接从开发的系统中受益的人、发现所有可能的需求源<br>需求获取前的准备：<br>明确问题清单（调研提纲）<br>    + 问题类型：封闭性问题、开放性问题<br>需求获取技术：<br>    + 访谈（领域知识、沟通能力）<br>    + 问卷调查（半封闭性问题：问卷设计（问题）、小范围验证）<br>    + 观察（演示）</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 理论 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 软件需求分析设计 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>软件需求分析设计-1</title>
      <link href="/2019/09/25/software-requirement-1/"/>
      <url>/2019/09/25/software-requirement-1/</url>
      
        <content type="html"><![CDATA[<h1 id="软件需求分析设计"><a href="#软件需求分析设计" class="headerlink" title="软件需求分析设计"></a>软件需求分析设计</h1><h2 id="第一章"><a href="#第一章" class="headerlink" title="第一章"></a>第一章</h2><h3 id="项目："><a href="#项目：" class="headerlink" title="项目："></a>项目：</h3><p>为创建一个独特产品、服务或任务所做的临时性努力</p><h3 id="软件开发通用技术："><a href="#软件开发通用技术：" class="headerlink" title="软件开发通用技术："></a>软件开发通用技术：</h3><p>1.抽象技术<br>2.分解技术<br>3.形式化表达（模型/模式、传播）<br>4.封装  </p><h3 id="软件开发方法："><a href="#软件开发方法：" class="headerlink" title="软件开发方法："></a>软件开发方法：</h3><p>1.结构化方法<br>2.面向对象方法：<br>    + 面向过程<br>    + 面向对象<br>    + 面向数据<br>    + 面向服务<br>    + 面向控制<br>面向对象和结构化方法的区别：<br>    + 面向对象按照对象和概念分解<br>    + 结构化方法按照过程和功能分解<br>    功能：抽象：抽取共性、抛弃不同特性的过程<br>面向对象的核心是面向接口<br>面向对象：可维护、可复用、可扩展、灵活性好</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 理论 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 软件需求分析设计 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>人月神话第三章：外科手术队伍</title>
      <link href="/2019/09/17/man-month-thirdchapter/"/>
      <url>/2019/09/17/man-month-thirdchapter/</url>
      
        <content type="html"><![CDATA[<p><img alt="外科手术现场" data-src="https://pic.superbed.cn/item/5d80cb26451253d1784b34c8.jpg" class="lozad"></p><h1 id="外科手术队伍"><a href="#外科手术队伍" class="headerlink" title="外科手术队伍"></a>外科手术队伍</h1><p>如何在有意义的进度安排内创建大型的系统？</p><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>协调矛盾： 对于效率和概念的完整性来说，最好由少数干练的人员来开发设计；对于大型系统来说，则需要大量的人手，以使产品能在时间上满足要求，怎么才能协调两者之间的矛盾？</p><h2 id="Mills的建议"><a href="#Mills的建议" class="headerlink" title="Mills的建议"></a>Mills的建议</h2><h4 id="10人编程团队："><a href="#10人编程团队：" class="headerlink" title="10人编程团队："></a>10人编程团队：</h4><h5 id="首席程序员（外科医生）："><a href="#首席程序员（外科医生）：" class="headerlink" title="首席程序员（外科医生）："></a>首席程序员（外科医生）：</h5><p>亲自定义功能和性能技术说明书，设计程序，编写源代码，测试以及书写技术文档。</p><h5 id="副手："><a href="#副手：" class="headerlink" title="副手："></a>副手：</h5><p>是外科医生的后备，能够完成任何一部分工作，但是相对经验较少。主要作为设计的思考者、讨论者和评估者。充当外科医生的保险机制。</p><h5 id="管理员："><a href="#管理员：" class="headerlink" title="管理员："></a>管理员：</h5><p>控制财务、人员、工作地点、办公设备的专业人才。</p><h5 id="编辑："><a href="#编辑：" class="headerlink" title="编辑："></a>编辑：</h5><p>根据外科医生的口述或草稿，进行分析和重新组织，提供各种参考信息和书目，对多个版本进行维护，并监督文档生成的机制。</p><h5 id="两个文秘："><a href="#两个文秘：" class="headerlink" title="两个文秘："></a>两个文秘：</h5><p>管理员和编辑都需要。</p><h5 id="程序职员："><a href="#程序职员：" class="headerlink" title="程序职员："></a>程序职员：</h5><p>负责编程产品库中所有团队的技术记录。</p><h5 id="工具维护人员："><a href="#工具维护人员：" class="headerlink" title="工具维护人员："></a>工具维护人员：</h5><h5 id="测试人员："><a href="#测试人员：" class="headerlink" title="测试人员："></a>测试人员：</h5><p>负责计划测试步骤等等</p><h5 id="语言专家："><a href="#语言专家：" class="headerlink" title="语言专家："></a>语言专家：</h5><p>掌握复杂语言</p><p>向所有团队成员展示所有计算机的运行和产物，并将所有的程序和数据看作是团队的所有物，而不是私有财产。Mills观念的关键在于：从个人艺术到公共实践。</p><h2 id="如何运作"><a href="#如何运作" class="headerlink" title="如何运作"></a>如何运作</h2><p>首席程序员和副手都了解所有的设计和全部代码，节省了很大的劳动量，确保了工作概念的完整性。<br>观点不一致之处由首席程序员单方面统一，达到客观一致性。</p><h2 id="团队的扩建"><a href="#团队的扩建" class="headerlink" title="团队的扩建"></a>团队的扩建</h2><p>要有一个系统结构师从上之下进行所有的设计，必须清晰的划分体系结构设计和实现之间的界线。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人月神话 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>人月神话第二章：人月神话</title>
      <link href="/2019/09/17/man-month-secondchapter/"/>
      <url>/2019/09/17/man-month-secondchapter/</url>
      
        <content type="html"><![CDATA[<p><img alt="新奥尔良Antoine餐厅菜单" data-src="https://pic.superbed.cn/item/5d80afc8451253d1784737bd.jpg" class="lozad"></p><h1 id="人月神话"><a href="#人月神话" class="headerlink" title="人月神话"></a>人月神话</h1><p>项目滞后的最主要原因：<br>缺乏合理的进度安排。<br>（对估算技术缺乏有效的研究、<br>我们所采用的估算技术隐含的假设人和月可以互换，即时间与工作人员数量可以替换、<br>对自己的估算缺乏信心、<br>对进度缺少跟踪和监督、<br>当意识到进度偏移时，下意识以及传统的反应是增加人力，如同汽油灭火，注定导致灾难。）</p><h2 id="乐观主义"><a href="#乐观主义" class="headerlink" title="乐观主义"></a>乐观主义</h2><p>所有编程人员都是乐观主义者。无论什么程序，结果都是毋庸置疑的：“这次它肯定能够运行”、“我刚刚找出了最后一个错误”。<br>导致了系统编程的进度安排背后的第一个错误的假设是：一切都将运作良好，每一项任务仅花费它所“应该”花费的时间。<br>《创造者思想》将创造性活动分为：构思、实现、交流。<br>正是由于介质的易于驾驭，我们期待在实现过程钟不会遇到困难，因此造成了乐观主义的弥漫。但我们的构思又是有缺陷的，总会发现bug。所以我们的乐观主义不该是理所当然的。</p><h2 id="人月"><a href="#人月" class="headerlink" title="人月"></a>人月</h2><p>第二个错误的思考方式是在估计和进度安排钟使用的工作量单位：人月，认为人数和时间可以互换。<br>但这种思想仅适用于某个任务可以分解为若干个小任务，且他们之间不需要任何交流。而这在编程过程中几乎不可能实现。<br>软件开发本质上是一项系统工作：错综复杂关系下的一种实践，沟通、交流的工作量非常大，它会很快分解任务分解所节省下来的个人时间。因此，增加更多的人手，实际上是延长了而不是缩短了时间进度。</p><h2 id="系统测试"><a href="#系统测试" class="headerlink" title="系统测试"></a>系统测试</h2><p>理论上，缺陷的数量应该为0，但是由于我们的乐观主义，实际上出现的缺陷数量要远远多于预料中的数量。因此，系统测试进度的安排往往是编程中最不合理的部分。<br>brooks经验法则:<br>1/3计划<br>1/6编码<br>1/4构建测试和早期系统测试<br>1/4系统测试，所有构件已完成</p><h2 id="空泛的估算"><a href="#空泛的估算" class="headerlink" title="空泛的估算"></a>空泛的估算</h2><p>某项人物的计划进度，可能受限于顾客要求的紧迫程度，但紧迫程度无法控制实际的进度。<br>两种解决方案：<br>开发推行生产率图表、缺陷率图表、估算规则等等；<br>或者，在可靠的估算出现前，项目经理坚持他们的估计。</p><h2 id="重复产生的进度灾难"><a href="#重复产生的进度灾难" class="headerlink" title="重复产生的进度灾难"></a>重复产生的进度灾难</h2><p>Brooks法则：<br>向进度落后的项目增加人手，只会使进度更加落后。</p><h5 id="除去了神话色彩的人月。项目的时间依赖于顺序上的限制，人员的最大数量依赖于独立子任务的数量。"><a href="#除去了神话色彩的人月。项目的时间依赖于顺序上的限制，人员的最大数量依赖于独立子任务的数量。" class="headerlink" title="除去了神话色彩的人月。项目的时间依赖于顺序上的限制，人员的最大数量依赖于独立子任务的数量。"></a>除去了神话色彩的人月。项目的时间依赖于顺序上的限制，人员的最大数量依赖于独立子任务的数量。</h5><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人月神话 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>人月神话第一章：焦油坑</title>
      <link href="/2019/09/17/man-month-firstchapter/"/>
      <url>/2019/09/17/man-month-firstchapter/</url>
      
        <content type="html"><![CDATA[<p><img alt="拉布雷阿的焦油坑壁画" data-src="https://pic.superbed.cn/item/5d8062b9451253d1783e6769.jpg" class="lozad"></p><h1 id="焦油坑"><a href="#焦油坑" class="headerlink" title="焦油坑"></a>焦油坑</h1><p>程序开发就好像是一个焦油坑，很多很多人在挣扎，却是无法摆脱束缚，避免沉到坑底的命运。</p><h2 id="编程系统产品"><a href="#编程系统产品" class="headerlink" title="编程系统产品"></a>编程系统产品</h2><p>单单讲编程一个程序并不难,但要将它转化为更有用的，则需要投入几倍的成本来达到。</p><h2 id="编程乐趣"><a href="#编程乐趣" class="headerlink" title="编程乐趣"></a>编程乐趣</h2><p>   它是一种创建事物的纯粹快乐，这种快乐是上帝创造世界的折射，一种呈现在每片独特的、崭新<br>的树叶和雪花上的喜悦。<br>   快乐还来源于我们是开发出对别人有用的东西。内心深处，我们渴望着我们的劳动成果被其他人<br>认同、使用。<br>   快乐来自于将互相啮合的零部件组合在一起，看它们精妙运行的成就感。<br>   这种快乐还是持续学习的快乐。我们面临的问题总是不尽相同的，可以在解决问题的过程中不断<br>学习到新的知识。<br>   最后，快乐还来自于我们是在一种极易驾驭的介质上工作。我们几乎可以完全凭借自己的想象，<br>来搭建自己快乐“城堡”。<br>   我们在键盘上键入不同的咒语，屏幕上就会活动，变化，呈现出前所未有的不可能存在的事物，<br>编程满足了我们内心深处的创造欲。</p><h2 id="编程苦恼"><a href="#编程苦恼" class="headerlink" title="编程苦恼"></a>编程苦恼</h2><p>   我们追求完美。我们将做事的方向不断向完美调整。<br>   由他人设定目标，供给资源，提供信息。我们依赖别人的程序，即使那些程序设计并不合理。<br>   不断修正，寻找bug。十分的枯燥乏味。<br>   苦恼抑或是无奈。当一个项目将要完成时，有可能已经过时，不再适合当下。</p><h5 id="这就是编程，令人痛苦挣扎的焦油坑，一种乐趣与苦恼共存的活动。"><a href="#这就是编程，令人痛苦挣扎的焦油坑，一种乐趣与苦恼共存的活动。" class="headerlink" title="这就是编程，令人痛苦挣扎的焦油坑，一种乐趣与苦恼共存的活动。"></a>这就是编程，令人痛苦挣扎的焦油坑，一种乐趣与苦恼共存的活动。</h5><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人月神话 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>木心先生诗选</title>
      <link href="/2019/09/11/muxin/"/>
      <url>/2019/09/11/muxin/</url>
      
        <content type="html"><![CDATA[<p>1</p><p>很多人的失落，是违背了自己少年时的立志。自认为成熟、自认为练达、自认为精明，从前多幼稚，总算看透了、想穿了。于是，我们就此变成自己年少时最憎恶的那种人。</p><p>——木心 《鱼丽之宴》</p><p>（不远千里多年，赴一个本不忍的约会……）</p><p>2</p><p>万头攒动火树银花之处不必找我。如欲相见，我在各种悲喜交集处，能做的只是长途跋涉的归真返璞。</p><p>——木心 《我纷纷的情欲》</p><p>（人间至色是素淡，人间至味是清欢）</p><p>3</p><p>看清世界荒谬，是一个智者的基本水准。看清了，不是感到恶心，而是会心一笑。</p><p>——木心</p><p>（暗暗摇头，三两声笑）</p><p>4</p><p>生命好在无意义，才容得下各自赋予意义。假如生命是有意义的，这个意义却不合我的志趣，那才尴尬狼狈。</p><p>——木心 《素履之往》</p><p>（无径自取径，苦也是我的苦，纵苦也欢愉）</p><p>5</p><p>有人说，时间是最妙的疗伤药。此话没说对，反正时间不是药，药在时间里。</p><p>——木心 《艾华利好兄弟》</p><p>（总之，时间还是治愈的……）</p><p>6</p><p>无知的人总是薄情的。无知的本质，就是薄情。</p><p>——木心 《文学回忆录》</p><p>（已是浅薄心，无处放深情）</p><p>7</p><p>昨天我和她坐在街头的喷泉边，五月的天气已很热了，刚买来的一袋樱桃也不好吃，我们抽着烟，“应该少抽烟才对”。</p><p>满街的人来来往往，她信口叹问：“生命是什么呵？”我脱口答道：“生命是时时刻刻不知如何是好。”</p><p>——木心</p><p>（四季三餐，如何是好）</p><p>8</p><p>不谦而狂的人，狂不到哪里去；不狂而谦的人，真不知其在谦什么。</p><p>——木心 《琼美卡随想录》</p><p>（收下我的膝盖……）</p><p>9</p><p>雨后总像有谁离去了。</p><p>——木心 《素履之往》</p><p>（雨后寥落，若有亡失）</p><p>10</p><p>常以为人是一种容器，盛着快乐，盛着悲哀。但人不是容器，人是导管，快乐流过，悲哀流过，导管只是导管。各种快乐悲哀流过流过，一直到死，导管才空了。疯子就是导管的淤塞和破裂。</p><p>——木心 《同车人的啜泣》</p><p>（过的就过了，没过的，就塌陷了……）</p><p>11</p><p>悲伤有很多种，能加以抑制的悲伤，未必称得上悲伤。</p><p>——木心 《即兴判断》</p><p>（正如，能加以抑制的真情，未必称得上真情）</p><p>12</p><p>规律背后，有命运在冷笑。</p><p>——木心</p><p>（除了敬畏，别无其他）</p><p>13</p><p>一流的情人永远不必殉陨，永远不会失恋，因为“我爱你，与你何涉。”</p><p>——木心</p><p>（爱你，我的爱情便成了）</p><p>14</p><p>没有比粥更温柔的了。念予毕生流离红尘，就找不到一个似粥温柔的人。</p><p>——木心 《少年朝食》</p><p>（未有粥一般的人，甚或，连粥一般的话都无）</p><p>15</p><p>爱情，亦三种境界耳。少年出乎好奇，青年在于审美，中年归向求知。老之将至，义无反顾。</p><p>——木心 《即兴判断》</p><p>（到老来，却是清醒深情）</p><p>16</p><p>负心，不奇。奇的是负心之前的一片真心</p><p>——木心 《云雀叫了一整天》</p><p>（因有负心为根，方显真心丑恶）</p><p>17</p><p>一个人，随便走几步，性格毕露。</p><p>——木心 《云雀叫了一整天》</p><p>（他的举止里，全是言语）</p><p>18</p><p>无论蓬户荆扉，都将因你的倚闾而成为我的凯旋门。</p><p>——木心</p><p>（世事本无意义，因你面目全非）</p><p>19</p><p>一场梦，不怨也不恨，上了想象力的当。</p><p>——木心 《温莎墓园日记》</p><p>（爱憎，莫扰无底色的梦）</p><p>20</p><p>没有自我的人的自我感觉都特别良好。</p><p>——木心 《素履之往》</p><p>（天大地大，自大最大）</p><p>21</p><p>有人一看书就卖弄，多看几遍再卖弄吧……多看几遍就不卖弄了。</p><p>——木心</p><p>（越多知，越懂无知）</p><p>23</p><p>文字的简练来自内心的真诚。我十二万分的爱你，就不如我爱你。</p><p>——木心</p><p>（爱已经太长，话就短些吧）</p><p>24</p><p>爱一个人，没有机会表白，后来决计绝念。再后来，消息时有所闻，偶尔也见面。幸亏那时未曾说出口，幸亏究竟不能算真的爱上。又爱了另一个人，表白的机会不少，想想，懒下来，懒成朋友，至今还朋友着。光阴荏苒，在电话里有说有笑，心中兀自庆幸，还好……否则苦了。</p><p>——木心 《素履之往》</p><p>（错过得并无遗憾，错过得只剩美感）</p><p>25</p><p>当愚人来找你商量事体，你别费精神——他早就定了主意的。</p><p>——木心 《素履之往》</p><p>（无谓的苦口婆心，不如沉默是金）</p><p>26</p><p>给他们面子是我自己要面子。</p><p>——木心 《云雀叫了一整天》</p><p>27</p><p>眼看一个个有志青年，熟门熟路地堕落了，许多“个人”加起来，便是“时代”。</p><p>——木心 《素履之往》</p><p>（低头自问，是否在时代的驼背之上，加了一根小小的稻草）</p><p>28</p><p>晴秋上午，随便走走，不一定要快乐。</p><p>——木心 《素履之往》</p><p>（字词自己腾跃，并不非得意义）</p><p>29</p><p>轻浮，随遇而爱，谓之滥情。多方向，无主次地泛恋，谓之滥情。言过其实，炫耀伎俩，谓之滥情。没条件的痴心忠于某一人，亦谓之滥情。</p><p>——木心 《素履之往》</p><p>（没有尺度，都是坍塌）</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 木心 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello Hexo</title>
      <link href="/2019/09/11/hello-world/"/>
      <url>/2019/09/11/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Hello-hexo！！"><a href="#Hello-hexo！！" class="headerlink" title="Hello hexo！！"></a>Hello hexo！！</h3><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class=" language-bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class=" language-bash"><code class="language-bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class=" language-bash"><code class="language-bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class=" language-bash"><code class="language-bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Hello Hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hello </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
